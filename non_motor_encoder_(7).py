# -*- coding: utf-8 -*-
"""non motor encoder (7).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bTuv8rxJWxFCuYe905OkfwjLSB2prCmu
"""

"""
Corrected Non-Motor Encoder - 16-DIM OUTPUT
Matches paper claims: LSTM-based encoder with 16-dimensional common output
"""

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import Adam
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import pickle

# ============================================================================
# CORRECTED NON-MOTOR ENCODER (16-DIM COMMON OUTPUT)
# ============================================================================

class CategoryEncoder(nn.Module):
    """LSTM encoder for each non-motor category"""
    def __init__(self, input_dim, embed_dim):
        super().__init__()
        hidden = embed_dim // 2
        self.lstm = nn.LSTM(input_dim, hidden, bidirectional=True, batch_first=True)
        self.mu = nn.Linear(embed_dim, embed_dim)
        self.logvar = nn.Linear(embed_dim, embed_dim)

    def reparam(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            return mu + std * torch.randn_like(std)
        return mu

    def forward(self, x):
        h, _ = self.lstm(x)
        mu, logvar = self.mu(h), self.logvar(h)
        return self.reparam(mu, logvar), mu, logvar


class NonMotorEncoder(nn.Module):
    """
    Non-Motor Encoder with 16-dimensional common output
    Paper: "16-dimensional latent space" per modality
    """
    def __init__(self, input_dims, embed_dims, common_dim=16):
        super().__init__()
        self.encoders = nn.ModuleList([CategoryEncoder(dim, ed) for dim, ed in zip(input_dims, embed_dims)])
        self.projectors = nn.ModuleList([nn.Linear(ed, common_dim) for ed in embed_dims])
        self.common_dim = common_dim

    def forward(self, x_list):
        zs, mus, logs = [], [], []
        for encoder, proj, x in zip(self.encoders, self.projectors, x_list):
            z, mu, logvar = encoder(x)
            zs.append(proj(z))
            mus.append(mu)
            logs.append(logvar)

        # Fuse across modalities: [batch, seq_len, common_dim]
        fused = torch.stack(zs, dim=2).mean(dim=2)

        # Pool across time to get fixed-size embedding: [batch, common_dim=16]
        pooled = fused.mean(dim=1)

        return pooled, mus, logs


class StagePredictor(nn.Module):
    def __init__(self, input_dims, embed_dims, common_dim=16, n_stages=3):
        super().__init__()
        self.encoder = NonMotorEncoder(input_dims, embed_dims, common_dim)
        self.classifier = nn.Linear(common_dim, n_stages)

    def forward(self, x_list):
        pooled, mus, logs = self.encoder(x_list)
        logits = self.classifier(pooled)
        return logits, pooled, mus, logs


# ============================================================================
# DATASET
# ============================================================================

class ParkinsonsDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = sequences
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def kl_div(mu, logvar):
    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())


def evaluate(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []
    total_loss = 0

    with torch.no_grad():
        for x_list, y in dataloader:
            x_list = [x.to(device) for x in x_list]
            y = y.to(device)
            logits, pooled, mus, logs = model(x_list)

            ce = F.cross_entropy(logits, y)
            kld = sum(kl_div(mus[i], logs[i]) for i in range(len(mus)))
            loss = ce + 0.1 * kld
            total_loss += loss.item()

            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='macro')
    cm = confusion_matrix(all_labels, all_preds)
    avg_loss = total_loss / len(dataloader)
    return avg_loss, acc, f1, cm


# ============================================================================
# PREPROCESSING FUNCTIONS
# ============================================================================

def preprocess_keep_numeric(df, drop_cols):
    df = df.sort_values(['PATNO', 'EVENT_ID'])
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')
    feat_cols = [c for c in df.columns if c not in ['PATNO', 'EVENT_ID']]
    df[feat_cols] = df[feat_cols].apply(pd.to_numeric, errors='coerce').fillna(0).astype('float32')
    return df, feat_cols


def create_sequences(df, feat_cols, max_seq_len=10):
    seqs, pat_list = [], []
    for pid, grp in df.groupby('PATNO'):
        data = grp.sort_values('EVENT_ID')[feat_cols].to_numpy(dtype=np.float32)
        t = torch.tensor(data)
        if t.size(0) < max_seq_len:
            pad = torch.zeros(max_seq_len - t.size(0), t.size(1))
            t = torch.cat([t, pad], dim=0)
        else:
            t = t[:max_seq_len]
        seqs.append(t)
        pat_list.append(pid)
    return seqs, pat_list


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("NON-MOTOR ENCODER (16-DIM COMMON OUTPUT)")
    print("="*70)

    # Load data
    print("\nLoading non-motor data...")
    df_cog = pd.read_parquet('cog.parquet', engine='pyarrow')
    df_psych = pd.read_parquet('merged_df.parquet', engine='pyarrow')
    df_sleep = pd.read_parquet('df2_Selected.parquet', engine='pyarrow')
    df_auto = pd.read_parquet('Merged_Df.parquet', engine='pyarrow')
    df_sens = pd.read_parquet('sleep.parquet', engine='pyarrow')

    # Phase mapping
    visit_phase_mapping = {
        'BL':'early','R17':'early','SC':'early','R16':'early','R15':'early','R13':'early',
        'LOG':'early','R08':'early','R10':'early','R12':'early','R06':'early','R04':'early',
        'R01':'early','PW':'early',
        'TRANS':'mid','V01':'mid','V02':'mid','V03':'mid','V04':'mid','V05':'mid','V06':'mid',
        'ST':'mid','RS1':'mid','R21':'mid','U02':'mid','U01':'mid','R19':'mid','R20':'mid','R14':'mid',
        'V07':'late','V08':'late','V09':'late','V10':'late','V11':'late','V12':'late','V13':'late',
        'V14':'late','V15':'late','V16':'late','V17':'late','V18':'late','V19':'late',
        'V20':'late','V21':'late','V22':'late','R18':'late'
    }
    stage_mapping = {'early':0,'mid':1,'late':2}

    # Consolidate labels
    all_events = pd.concat([df[['PATNO','EVENT_ID']] for df in [df_cog,df_psych,df_sleep,df_auto,df_sens]]).drop_duplicates()
    all_events['stage_str'] = all_events['EVENT_ID'].map(visit_phase_mapping)
    latest = all_events.sort_values('EVENT_ID').groupby('PATNO').tail(1).reset_index(drop=True)
    labels_df = latest[['PATNO','stage_str']]
    labels_df['label'] = labels_df['stage_str'].map(stage_mapping)

    # Preprocess data
    non_numeric_cols = [
        'REC_ID_x','PAG_NAME_x','INFODT_x','BIRTHDT','CLIA','GWAS','WES','WGS','SVs','SANGER',
        'IU_Fingerprint','RNASEQ','APOE','VAR_GENE','LRRK2','GBA','VPS35','SNCA','PRKN','PARK7',
        'PINK1','NOTES','COHORT_DEFINITION','ENROLL_DATE','ENROLL_STATUS','STATUS_DATE','INEXPAGE',
        'PPMI_ONLINE_ENROLL','EducationCountry','EducationLevel','REC_ID_y','PAG_NAME_y','INFODT_y',
        'ORIG_ENTRY','LAST_UPDATE','visit_phase'
    ]

    dfs = [df_cog, df_psych, df_sleep, df_auto, df_sens]
    processed, feature_cols_list = [], []
    for df in dfs:
        df2, fcols = preprocess_keep_numeric(df, non_numeric_cols)
        processed.append(df2)
        feature_cols_list.append(fcols)

    # Create sequences
    seqs_list, patno_lists = zip(*[create_sequences(df, cols) for df,cols in zip(processed,feature_cols_list)])
    all_patnos = sorted(set().union(*patno_lists))
    aligned_seqs = []
    for i, (mod_seqs, mod_patnos) in enumerate(zip(seqs_list, patno_lists)):
        pid_to_seq = dict(zip(mod_patnos, mod_seqs))
        aligned = [pid_to_seq.get(pid, torch.zeros_like(mod_seqs[0])) for pid in all_patnos]
        aligned_seqs.append(aligned)
    seqs_list = aligned_seqs
    patient_ids = all_patnos

    # Final sequences
    sequences = [tuple(mod[i] for mod in seqs_list) for i in range(len(patient_ids))]

    # Labels
    label_map = dict(zip(labels_df['PATNO'], labels_df['label']))
    labels = [label_map.get(pid, 0) for pid in patient_ids]

    print(f"Total patients: {len(patient_ids)}")
    print(f"Label distribution: {np.bincount(labels)}")

    # Dataset
    dataset = ParkinsonsDataset(sequences, labels)

    # Load master split
    print("\nLoading master patient split...")
    with open('master_patient_split.pkl', 'rb') as f:
        split_data = pickle.load(f)

    train_patients = set(split_data['train_patients'])
    val_patients = set(split_data['val_patients'])
    test_patients = set(split_data['test_patients'])

    # Map indices
    train_indices = [i for i, pid in enumerate(patient_ids) if pid in train_patients]
    val_indices = [i for i, pid in enumerate(patient_ids) if pid in val_patients]
    test_indices = [i for i, pid in enumerate(patient_ids) if pid in test_patients]

    print(f"Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}")

    # Verify no leakage
    train_pats = set([patient_ids[i] for i in train_indices])
    val_pats = set([patient_ids[i] for i in val_indices])
    test_pats = set([patient_ids[i] for i in test_indices])

    assert len(train_pats & val_pats) == 0, "Leakage: train-val!"
    assert len(train_pats & test_pats) == 0, "Leakage: train-test!"
    assert len(val_pats & test_pats) == 0, "Leakage: val-test!"
    print("✓ No patient leakage\n")

    # Data loaders
    train_subset = torch.utils.data.Subset(dataset, train_indices)
    val_subset = torch.utils.data.Subset(dataset, val_indices)
    test_subset = torch.utils.data.Subset(dataset, test_indices)

    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_subset, batch_size=16, shuffle=False)
    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)

    # Model
    input_dims = [len(cols) for cols in feature_cols_list]
    embed_dims = [16, 16, 12, 12, 8]
    COMMON_DIM = 16  # CRITICAL: Changed to 16

    print(f"Input dimensions: {input_dims}")
    print(f"Embed dimensions: {embed_dims}")
    print(f"Common dimension: {COMMON_DIM} (matches paper)")

    model = StagePredictor(input_dims, embed_dims, common_dim=COMMON_DIM, n_stages=3)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    model.to(device)
    optimizer = Adam(model.parameters(), lr=1e-3)

    # Training
    print("\nTraining non-motor encoder...")
    best_val_f1 = 0.0
    patience = 10
    patience_counter = 0

    for epoch in range(1, 51):
        model.train()
        total_loss = 0
        for x_list, y in train_loader:
            x_list = [x.to(device) for x in x_list]
            y = y.to(device)
            optimizer.zero_grad()
            logits, pooled, mus, logs = model(x_list)
            ce = F.cross_entropy(logits, y)
            kld = sum(kl_div(mus[i], logs[i]) for i in range(len(mus)))
            loss = ce + 0.1 * kld
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_loader)
        val_loss, val_acc, val_f1, val_cm = evaluate(model, val_loader, device)

        if epoch % 5 == 0:
            print(f"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                  f"Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), 'best_nonmotor_model.pth')
            print(f"  → Saved best model (F1: {best_val_f1:.3f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break

    # Load best
    model.load_state_dict(torch.load('best_nonmotor_model.pth', weights_only=True))

    # Final evaluation
    print("\n" + "="*50)
    print("Final Validation Evaluation:")
    print("="*50)
    val_loss, val_acc, val_f1, val_cm = evaluate(model, val_loader, device)
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
    print(f"\nConfusion Matrix:\n{val_cm}")

    # Per-class metrics
    all_preds, all_labels = [], []
    model.eval()
    with torch.no_grad():
        for x_list, y in val_loader:
            x_list = [x.to(device) for x in x_list]
            logits, _, _, _ = model(x_list)
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())

    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, labels=range(3)
    )
    class_names = ['early', 'mid', 'late']
    for i, cls in enumerate(class_names):
        print(f"Class '{cls}': Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}")

    # Save encoder
    torch.save(model.encoder.state_dict(), 'nonmotor_encoder_weights.pth')
    print("\n✓ Non-motor encoder weights saved")
    print(f"✓ Output dimension: {COMMON_DIM}")
    print(f"✓ Input dimensions: {input_dims}")

