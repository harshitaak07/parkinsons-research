# -*- coding: utf-8 -*-
"""motor encoder (6).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8CBNwmme6G1jwXDyUqfKUBOjFwZj_lb
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from torch.optim.lr_scheduler import CosineAnnealingLR
from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# ============================================================================
# CORRECTED TRANSFORMER-BASED MOTOR ENCODER (16-DIM OUTPUT)
# ============================================================================

class TransformerMotorEncoder(nn.Module):
    """
    Transformer-based Motor Encoder
    Paper claims: "transformer encoders" with 16-dimensional latent output
    """
    def __init__(self, input_dim=14, latent_dim=16, num_heads=4, num_layers=2, dropout=0.4):
        super().__init__()

        # Embedding layer
        self.embedding = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.Dropout(dropout)
        )

        # Transformer encoder layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=latent_dim,
            nhead=num_heads,
            dim_feedforward=latent_dim * 4,
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Output normalization
        self.output_norm = nn.LayerNorm(latent_dim)

    def forward(self, x):
        """
        Args:
            x: [batch, input_dim] motor features
        Returns:
            latent: [batch, latent_dim=16] encoded representation
        """
        # Add sequence dimension: [batch, 1, latent_dim]
        x = self.embedding(x.unsqueeze(1))

        # Apply transformer
        x = self.transformer(x)

        # Pool and normalize: [batch, latent_dim]
        x = x.squeeze(1)
        latent = self.output_norm(x)

        return latent


class MotorClassifier(nn.Module):
    """Motor classifier with Transformer encoder"""
    def __init__(self, input_dim=14, latent_dim=16, num_stages=3):
        super().__init__()
        self.encoder = TransformerMotorEncoder(input_dim, latent_dim)

        # Classifier head
        self.classifier = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(latent_dim, 32),
            nn.GELU(),
            nn.BatchNorm1d(32),
            nn.Dropout(0.3),
            nn.Linear(32, num_stages)
        )

    def forward(self, x):
        latent = self.encoder(x)
        logits = self.classifier(latent)
        return logits, latent


# ============================================================================
# DATASET CLASS
# ============================================================================

class MotorDatasetWithLabels(Dataset):
    def __init__(self, df, symptom_cols, stage_col='Combined_Stage', stage_map=None):
        self.df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=symptom_cols + [stage_col])
        scaler = StandardScaler()
        self.features = torch.tensor(scaler.fit_transform(self.df[symptom_cols].values), dtype=torch.float32)
        self.labels = torch.tensor(self.df[stage_col].map(stage_map).values, dtype=torch.long)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]


# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

def train_epoch(model, dataloader, optimizer, criterion, criterion_no_reduction, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    per_class_losses = {0: 0.0, 1: 0.0, 2: 0.0}
    per_class_counts = {0: 0, 1: 0, 2: 0}

    for features, labels in dataloader:
        features, labels = features.to(device), labels.to(device)
        optimizer.zero_grad()
        logits, _ = model(features)
        loss = criterion(logits, labels)
        loss.backward()

        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()
        total_loss += loss.item() * features.size(0)

        preds = torch.argmax(logits, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

        for cls in range(3):
            cls_mask = (labels == cls)
            if cls_mask.sum() > 0:
                losses_cls = criterion_no_reduction(logits[cls_mask], labels[cls_mask])
                per_class_losses[cls] += losses_cls.sum().item()
                per_class_counts[cls] += cls_mask.sum().item()

    avg_loss = total_loss / total
    accuracy = correct / total
    per_class_avg_losses = {cls: (per_class_losses[cls] / per_class_counts[cls] if per_class_counts[cls] > 0 else 0) for cls in per_class_losses}
    return avg_loss, accuracy, per_class_avg_losses


def evaluate(model, dataloader, criterion, criterion_no_reduction, device):
    model.eval()
    all_preds, all_labels, all_probs = [], [], []
    val_loss = 0
    per_class_losses = {0: 0.0, 1: 0.0, 2: 0.0}
    per_class_counts = {0: 0, 1: 0, 2: 0}

    with torch.no_grad():
        for features, labels in dataloader:
            features, labels = features.to(device), labels.to(device)
            logits, _ = model(features)
            loss = criterion(logits, labels)
            val_loss += loss.item() * features.size(0)
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            probs = torch.softmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs)

            for cls in range(3):
                cls_mask = (labels == cls)
                if cls_mask.sum() > 0:
                    losses_cls = criterion_no_reduction(logits[cls_mask], labels[cls_mask])
                    per_class_losses[cls] += losses_cls.sum().item()
                    per_class_counts[cls] += cls_mask.sum().item()

    val_loss /= len(dataloader.dataset)
    acc = np.mean(np.array(all_preds) == np.array(all_labels))
    f1 = f1_score(all_labels, all_preds, average='macro')
    roc_auc = roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr')
    report = classification_report(all_labels, all_preds, target_names=['Early', 'Mid', 'Late'], zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)
    per_class_avg_losses = {cls: (per_class_losses[cls] / per_class_counts[cls] if per_class_counts[cls] > 0 else 0) for cls in per_class_losses}
    return acc, f1, roc_auc, report, cm, val_loss, per_class_avg_losses


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("TRANSFORMER-BASED MOTOR ENCODER (16-DIM LATENT)")
    print("="*70)

    # Load data
    print("\nLoading motor data...")
    df_pyarrow_motor = pd.read_parquet('motor.parquet', engine='pyarrow')

    motor_symptom_cols = [
        'NP2SPCH', 'NP2SALV', 'NP2SWAL', 'NP2EAT', 'NP2DRES', 'NP2HYGN',
        'NP2HWRT', 'NP2HOBB', 'NP2TURN', 'NP2TRMR', 'NP2RISE', 'NP2WALK', 'NP2FREZ',
        'Total_Motor_Score'
    ]

    stage_map = {'Early': 0, 'Mid': 1, 'Late': 2}

    # Load master patient split
    print("Loading master patient split...")
    with open('master_patient_split.pkl', 'rb') as f:
        splits = pickle.load(f)

    train_patients = set(splits['train_patients'])
    val_patients = set(splits['val_patients'])
    test_patients = set(splits['test_patients'])

    # Create masks
    train_mask = df_pyarrow_motor['PATNO'].isin(train_patients)
    val_mask = df_pyarrow_motor['PATNO'].isin(val_patients)
    test_mask = df_pyarrow_motor['PATNO'].isin(test_patients)

    # Split data
    X_train = df_pyarrow_motor.loc[train_mask, motor_symptom_cols].values
    X_val = df_pyarrow_motor.loc[val_mask, motor_symptom_cols].values
    X_test = df_pyarrow_motor.loc[test_mask, motor_symptom_cols].values

    y_train = df_pyarrow_motor.loc[train_mask, 'Combined_Stage'].values
    y_val = df_pyarrow_motor.loc[val_mask, 'Combined_Stage'].values
    y_test = df_pyarrow_motor.loc[test_mask, 'Combined_Stage'].values

    print(f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")

    # Encode labels
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_val_encoded = le.transform(y_val)
    y_test_encoded = le.transform(y_test)

    # Balance validation set
    val_counts = dict(zip(*np.unique(y_val_encoded, return_counts=True)))
    min_samples = min(val_counts.values())
    val_indices = []
    for cls in np.unique(y_val_encoded):
        cls_indices = np.where(y_val_encoded == cls)[0]
        sampled_indices = np.random.choice(cls_indices, size=min_samples, replace=False)
        val_indices.extend(sampled_indices)
    val_indices = np.array(val_indices)
    X_val = X_val[val_indices]
    y_val_encoded = y_val_encoded[val_indices]

    # Impute
    imputer = SimpleImputer(strategy='mean')
    X_train_imputed = imputer.fit_transform(X_train)
    X_val_imputed = imputer.transform(X_val)
    X_test_imputed = imputer.transform(X_test)

    # Balance training with SMOTE
    under_strategy = {0: np.count_nonzero(y_train_encoded == 0),
                      1: np.count_nonzero(y_train_encoded == 1),
                      2: 4000}
    over_strategy = {0: 4000, 1: 7000, 2: 4000}

    sampling_pipeline = Pipeline([
        ('under', RandomUnderSampler(sampling_strategy=under_strategy, random_state=42)),
        ('smote', SMOTE(sampling_strategy=over_strategy, random_state=42))
    ])
    X_train_resampled, y_train_resampled = sampling_pipeline.fit_resample(X_train_imputed, y_train_encoded)

    print(f"After resampling: {len(X_train_resampled)}")
    print(f"Class distribution: {np.bincount(y_train_resampled)}")

    # Create datasets
    y_train_resampled_str = le.inverse_transform(y_train_resampled)
    y_val_str = le.inverse_transform(y_val_encoded)

    train_df = pd.DataFrame(np.column_stack([X_train_resampled, y_train_resampled_str]),
                            columns=motor_symptom_cols + ['Combined_Stage'])
    val_df = pd.DataFrame(np.column_stack([X_val_imputed, y_val_str]),
                          columns=motor_symptom_cols + ['Combined_Stage'])

    train_dataset = MotorDatasetWithLabels(train_df, motor_symptom_cols, stage_map=stage_map)
    val_dataset = MotorDatasetWithLabels(val_df, motor_symptom_cols, stage_map=stage_map)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)

    # Setup device and model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nUsing device: {device}")

    LATENT_DIM = 16  # CRITICAL: Changed from 64 to 16
    print(f"Latent dimension: {LATENT_DIM} (matches paper)")

    model = MotorClassifier(input_dim=len(motor_symptom_cols), latent_dim=LATENT_DIM, num_stages=3).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-2)
    scheduler = CosineAnnealingLR(optimizer, T_max=50)

    # Class weights
    class_counts = np.bincount(y_train_resampled)
    total_samples = len(y_train_resampled)
    class_weights = total_samples / (len(class_counts) * class_counts)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)

    criterion_ce = nn.CrossEntropyLoss(weight=class_weights)
    criterion_ce_no_reduction = nn.CrossEntropyLoss(weight=class_weights, reduction='none')

    # Training loop
    print("\nTraining transformer motor encoder...")
    train_losses, val_losses, train_accs, val_accs, val_f1s, val_roc_aucs = [], [], [], [], [], []

    best_val_f1 = 0.0
    patience, patience_counter = 40, 0

    for epoch in range(300):
        train_loss, train_acc, train_per_class = train_epoch(
            model, train_loader, optimizer, criterion_ce, criterion_ce_no_reduction, device
        )
        val_acc, val_f1, val_roc_auc, val_report, val_cm, val_loss, val_per_class = evaluate(
            model, val_loader, criterion_ce, criterion_ce_no_reduction, device
        )

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        val_f1s.append(val_f1)
        val_roc_aucs.append(val_roc_auc)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/300, Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.3f}, "
                  f"Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.3f}, Val F1: {val_f1:.3f}, Val AUC: {val_roc_auc:.3f}")

        scheduler.step()

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            torch.save(model.state_dict(), 'best_motor_classifier.pth')
            print(f"  → Saved best model (F1: {best_val_f1:.3f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

    # Load best model
    model.load_state_dict(torch.load('best_motor_classifier.pth', weights_only=True))

    # Final evaluation
    print("\n" + "="*50)
    print("Final Validation Evaluation:")
    print("="*50)
    val_acc, val_f1, val_roc_auc, val_report, val_cm, val_loss, val_per_class = evaluate(
        model, val_loader, criterion_ce, criterion_ce_no_reduction, device
    )
    print(f"Val Acc: {val_acc:.3f}, Val F1: {val_f1:.3f}, Val AUC: {val_roc_auc:.3f}")
    print(f"\nClassification Report:\n{val_report}")
    print(f"\nConfusion Matrix:\n{val_cm}")

    # Save encoder weights separately
    torch.save(model.encoder.state_dict(), 'motor_encoder_weights.pth')
    print("\n✓ Motor encoder weights saved to 'motor_encoder_weights.pth'")
    print(f"✓ Motor encoder output dimension: {LATENT_DIM}")
    print(f"✓ Motor encoder input dimension: {len(motor_symptom_cols)}")

