# -*- coding: utf-8 -*-
"""combined_encoder_with_attention (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nO4UM_0DtfN-bh4LhoyjZ0Vrbo8Xf-ZK
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile combined_encoder_with_attention.py
# """
# Combined Encoder with Attention-Based Fusion
# Combines pre-trained Motor, Non-Motor, and Imaging encoders
# """
# 
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import pandas as pd
# import numpy as np
# from torch.utils.data import Dataset
# from sklearn.preprocessing import StandardScaler
# import os
# 
# # ============================================================================
# # ENCODER ARCHITECTURES (16-DIM OUTPUT)
# # ============================================================================
# 
# class TransformerMotorEncoder(nn.Module):
#     def __init__(self, input_dim=14, latent_dim=16, num_heads=4, num_layers=2, dropout=0.4):
#         super().__init__()
#         self.embedding = nn.Sequential(
#             nn.Linear(input_dim, latent_dim),
#             nn.LayerNorm(latent_dim),
#             nn.Dropout(dropout)
#         )
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=latent_dim, nhead=num_heads, dim_feedforward=latent_dim*4,
#             dropout=dropout, activation='gelu', batch_first=True, norm_first=True
#         )
#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
#         self.output_norm = nn.LayerNorm(latent_dim)
# 
#     def forward(self, x):
#         x = self.embedding(x.unsqueeze(1))
#         x = self.transformer(x)
#         x = x.squeeze(1)
#         return self.output_norm(x)
# 
# 
# class CategoryEncoder(nn.Module):
#     """LSTM encoder for non-motor categories"""
#     def __init__(self, input_dim, embed_dim):
#         super().__init__()
#         hidden = embed_dim // 2
#         self.lstm = nn.LSTM(input_dim, hidden, bidirectional=True, batch_first=True)
#         self.mu = nn.Linear(embed_dim, embed_dim)
#         self.logvar = nn.Linear(embed_dim, embed_dim)
# 
#     def reparam(self, mu, logvar):
#         if self.training:
#             std = torch.exp(0.5 * logvar)
#             return mu + std * torch.randn_like(std)
#         return mu
# 
#     def forward(self, x):
#         h, _ = self.lstm(x)
#         mu, logvar = self.mu(h), self.logvar(h)
#         return self.reparam(mu, logvar), mu, logvar
# 
# 
# class NonMotorEncoder(nn.Module):
#     """Non-Motor Encoder - 16-dim common output"""
#     def __init__(self, input_dims, embed_dims, common_dim=16):
#         super().__init__()
#         self.encoders = nn.ModuleList([CategoryEncoder(dim, ed) for dim, ed in zip(input_dims, embed_dims)])
#         self.projectors = nn.ModuleList([nn.Linear(ed, common_dim) for ed in embed_dims])
#         self.common_dim = common_dim
# 
#     def forward(self, x_list):
#         zs, mus, logs = [], [], []
#         for encoder, proj, x in zip(self.encoders, self.projectors, x_list):
#             z, mu, logvar = encoder(x)
#             zs.append(proj(z))
#             mus.append(mu)
#             logs.append(logvar)
# 
#         fused = torch.stack(zs, dim=2).mean(dim=2)
#         pooled = fused.mean(dim=1)
#         return pooled, mus, logs
# 
# 
# class ImagingEncoder(nn.Module):
#     """Imaging Encoder - 16-dim output"""
#     def __init__(self, input_dim, latent_dim=16, hidden_dims=[256, 128], dropout_p=0.4):
#         super().__init__()
#         layers = []
#         prev_dim = input_dim
#         for h_dim in hidden_dims:
#             layers.append(nn.Linear(prev_dim, h_dim))
#             layers.append(nn.ReLU())
#             layers.append(nn.BatchNorm1d(h_dim))
#             layers.append(nn.Dropout(dropout_p))
#             prev_dim = h_dim
#         self.encoder = nn.Sequential(*layers)
#         self.fc_mu = nn.Linear(prev_dim, latent_dim)
#         self.fc_logvar = nn.Linear(prev_dim, latent_dim)
#         self.latent_dim = latent_dim
# 
#     def reparameterize(self, mu, logvar):
#         if self.training:
#             std = torch.exp(0.5 * logvar)
#             eps = torch.randn_like(std)
#             return mu + eps * std
#         return mu
# 
#     def forward(self, x):
#         h = self.encoder(x)
#         mu = self.fc_mu(h)
#         logvar = self.fc_logvar(h)
#         z = self.reparameterize(mu, logvar)
#         return z, mu, logvar
# 
# 
# # ============================================================================
# # ATTENTION-BASED FUSION
# # ============================================================================
# 
# class AttentionFusion(nn.Module):
#     """Attention-based fusion - outputs 48-dim (16 x 3)"""
#     def __init__(self, latent_dim=16, num_modalities=3, num_heads=4):
#         super().__init__()
#         self.latent_dim = latent_dim
#         self.num_modalities = num_modalities
# 
#         self.attention = nn.MultiheadAttention(
#             embed_dim=latent_dim,
#             num_heads=num_heads,
#             batch_first=True,
#             dropout=0.1
#         )
# 
#         self.norm = nn.LayerNorm(latent_dim)
#         self.fused_dim = latent_dim * num_modalities
# 
#     def forward(self, motor_emb, nonmotor_emb, imaging_emb):
#         stacked = torch.stack([motor_emb, nonmotor_emb, imaging_emb], dim=1)
#         attended, attn_weights = self.attention(stacked, stacked, stacked)
#         attended = self.norm(attended)
#         fused = attended.reshape(attended.size(0), -1)
#         return fused, attn_weights
# 
# 
# # ============================================================================
# # UNIFIED FUSED ENCODER
# # ============================================================================
# 
# class UnifiedFusedEncoder(nn.Module):
#     """Complete fused encoder with attention - 48-dim output"""
#     def __init__(self, motor_encoder, nonmotor_encoder, imaging_encoder,
#                  latent_dim=16, freeze_encoders=True):
#         super().__init__()
# 
#         self.motor_encoder = motor_encoder
#         self.nonmotor_encoder = nonmotor_encoder
#         self.imaging_encoder = imaging_encoder
#         self.latent_dim = latent_dim
# 
#         if freeze_encoders:
#             for param in self.motor_encoder.parameters():
#                 param.requires_grad = False
#             for param in self.nonmotor_encoder.parameters():
#                 param.requires_grad = False
#             for param in self.imaging_encoder.parameters():
#                 param.requires_grad = False
#             print("✓ Encoder weights frozen")
# 
#         self.fusion = AttentionFusion(latent_dim=latent_dim, num_modalities=3)
#         self.fused_dim = 48
# 
#         print(f"✓ Fusion: Attention-based")
#         print(f"✓ Fused dimension: {self.fused_dim}")
# 
#     def forward(self, motor_x, nonmotor_x_list, imaging_x):
#         motor_emb = self.motor_encoder(motor_x)
#         nonmotor_emb, mus, logs = self.nonmotor_encoder(nonmotor_x_list)
#         imaging_emb, img_mu, img_logvar = self.imaging_encoder(imaging_x)
# 
#         fused, attn_weights = self.fusion(motor_emb, nonmotor_emb, imaging_emb)
# 
#         individual_embeddings = {
#             'motor': motor_emb,
#             'nonmotor': nonmotor_emb,
#             'imaging': imaging_emb,
#             'attention_weights': attn_weights
#         }
# 
#         return fused, individual_embeddings
# 
#     def get_embedding_dim(self):
#         return self.fused_dim
# 
#     def unfreeze_encoders(self):
#         for param in self.motor_encoder.parameters():
#             param.requires_grad = True
#         for param in self.nonmotor_encoder.parameters():
#             param.requires_grad = True
#         for param in self.imaging_encoder.parameters():
#             param.requires_grad = True
#         print("✓ Encoders unfrozen")
# 
# 
# # ============================================================================
# # HELPER FUNCTIONS
# # ============================================================================
# 
# def infer_nonmotor_dims_from_checkpoint(checkpoint_path):
#     """Infer dimensions from checkpoint"""
#     checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)
# 
#     input_dims, embed_dims = [], []
#     i = 0
#     while f'encoders.{i}.lstm.weight_ih_l0' in checkpoint:
#         lstm_weight_shape = checkpoint[f'encoders.{i}.lstm.weight_ih_l0'].shape
#         input_size = lstm_weight_shape[1]
# 
#         mu_weight_shape = checkpoint[f'encoders.{i}.mu.weight'].shape
#         embed_dim = mu_weight_shape[0]
# 
#         input_dims.append(input_size)
#         embed_dims.append(embed_dim)
#         i += 1
# 
#     print(f"  Detected {len(input_dims)} non-motor modalities")
#     print(f"  Input dims: {input_dims}")
#     print(f"  Embed dims: {embed_dims}")
# 
#     return input_dims, embed_dims
# 
# 
# def load_pretrained_encoders(motor_weights_path='motor_encoder_weights.pth',
#                             nonmotor_weights_path='nonmotor_encoder_weights.pth',
#                             imaging_weights_path='imaging_encoder_weights.pth',
#                             motor_input_dim=14,
#                             nonmotor_input_dims=None,
#                             nonmotor_embed_dims=None,
#                             imaging_input_dim=None,
#                             latent_dim=16,
#                             device='cpu'):
#     """Load all pre-trained encoders"""
#     print("Loading pre-trained encoders...")
# 
#     motor_encoder = TransformerMotorEncoder(input_dim=motor_input_dim, latent_dim=latent_dim)
#     if os.path.exists(motor_weights_path):
#         motor_encoder.load_state_dict(torch.load(motor_weights_path, map_location=device, weights_only=True))
#         print(f"✓ Loaded motor encoder (16-dim)")
#     else:
#         print(f"⚠ Motor encoder not found, using random init")
# 
#     if os.path.exists(nonmotor_weights_path):
#         if nonmotor_input_dims is None or nonmotor_embed_dims is None:
#             nonmotor_input_dims, nonmotor_embed_dims = infer_nonmotor_dims_from_checkpoint(nonmotor_weights_path)
# 
#         nonmotor_encoder = NonMotorEncoder(nonmotor_input_dims, nonmotor_embed_dims, common_dim=latent_dim)
#         nonmotor_encoder.load_state_dict(torch.load(nonmotor_weights_path, map_location=device, weights_only=True))
#         print(f"✓ Loaded non-motor encoder (16-dim)")
#     else:
#         if nonmotor_input_dims is None:
#             nonmotor_input_dims = [101, 35, 13, 35, 21]
#         if nonmotor_embed_dims is None:
#             nonmotor_embed_dims = [16, 16, 12, 12, 8]
#         print(f"⚠ Non-motor encoder not found, using random init")
#         nonmotor_encoder = NonMotorEncoder(nonmotor_input_dims, nonmotor_embed_dims, common_dim=latent_dim)
# 
#     if imaging_input_dim is None:
#         raise ValueError("imaging_input_dim must be specified")
# 
#     imaging_encoder = ImagingEncoder(imaging_input_dim, latent_dim=latent_dim)
#     if os.path.exists(imaging_weights_path):
#         imaging_encoder.load_state_dict(torch.load(imaging_weights_path, map_location=device, weights_only=True))
#         print(f"✓ Loaded imaging encoder (16-dim)")
#     else:
#         print(f"⚠ Imaging encoder not found, using random init")
# 
#     return motor_encoder, nonmotor_encoder, imaging_encoder

print('ok')

