# -*- coding: utf-8 -*-
"""imaging encoder (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggjLDlbUdLVaapc-APungutoJtfsVGmv
"""

"""
Corrected Imaging Encoder - 16-DIM OUTPUT
Matches paper claims: 16-dimensional latent output
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import pickle

# ============================================================================
# CORRECTED IMAGING ENCODER (16-DIM OUTPUT)
# ============================================================================

class VariationalMLPEncoder(nn.Module):
    """
    Imaging Encoder with 16-dimensional latent output
    Paper: "16-dimensional latent space"
    """
    def __init__(self, input_dim, latent_dim=16, hidden_dims=[256, 128], dropout_p=0.4):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(h_dim))
            layers.append(nn.Dropout(dropout_p))
            prev_dim = h_dim
        self.encoder = nn.Sequential(*layers)
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)
        self.latent_dim = latent_dim

    def reparameterize(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5*logvar)
            eps = torch.randn_like(std)
            return mu + eps*std
        return mu

    def forward(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar


class ImagingEncoderClassifier(nn.Module):
    def __init__(self, encoder, latent_dim, n_classes):
        super().__init__()
        self.encoder = encoder
        self.classifier = nn.Linear(latent_dim, n_classes)

    def forward(self, x):
        z, mu, logvar = self.encoder(x)
        logits = self.classifier(z)
        return logits, z, mu, logvar


# ============================================================================
# DATASET
# ============================================================================

class CombinedImagingDataset(Dataset):
    def __init__(self, df, mri_cols, dti_cols):
        self.df = df.reset_index(drop=True)
        self.mri_cols = mri_cols
        self.dti_cols = dti_cols
        self.patient_ids = df['PATNO'].values

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        x_mri = torch.tensor(row[self.mri_cols].astype(float).values, dtype=torch.float32)
        x_dti = torch.tensor(row[self.dti_cols].astype(float).values, dtype=torch.float32)
        x_combined = torch.cat([x_mri, x_dti])
        y = torch.tensor(row['Stage_encoded'], dtype=torch.long)
        return x_combined, y


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def kl_divergence(mu, logvar):
    return -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)


def train_epoch(model, dataloader, optimizer, device, criterion, kl_weight=0.05):
    model.train()
    total_loss = 0
    for x, y in dataloader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        logits, z, mu, logvar = model(x)
        ce_loss = criterion(logits, y)
        kld_loss = kl_divergence(mu, logvar).mean()
        loss = ce_loss + kl_weight*kld_loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(dataloader)


def evaluate(model, dataloader, device, criterion, kl_weight=0.05):
    model.eval()
    preds, trues = [], []
    total_loss = 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            logits, z, mu, logvar = model(x)
            ce_loss = criterion(logits, y)
            kld_loss = kl_divergence(mu, logvar).mean()
            loss = ce_loss + kl_weight*kld_loss
            total_loss += loss.item()
            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
            trues.extend(y.cpu().numpy())
    acc = accuracy_score(trues, preds)
    f1 = f1_score(trues, preds, average='macro')
    cm = confusion_matrix(trues, preds)
    return total_loss / len(dataloader), acc, f1, cm, preds, trues


def plot_confusion_matrix(true_labels, pred_labels, classes, title='Confusion Matrix'):
    cm = confusion_matrix(true_labels, pred_labels)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(title)
    plt.tight_layout()
    plt.savefig(f'imaging_encoder_{title.lower().replace(" ", "_")}.png')
    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print("IMAGING ENCODER (16-DIM LATENT OUTPUT)")
    print("="*70)

    # Load DTI
    print("\nLoading imaging data...")
    dti = pd.read_parquet('dti.parquet')
    print(f"Raw DTI shape: {dti.shape}")

    # Pivot DTI
    dti_pivot = dti.pivot_table(
        index=['PATNO', 'Stage'],
        columns='Measure',
        values=['ROI1', 'ROI2', 'ROI3', 'ROI4', 'ROI5', 'ROI6', 'REF1', 'REF2']
    ).reset_index()
    dti_pivot.columns = ['_'.join(col).strip('_') for col in dti_pivot.columns.values]
    dti_pivot = dti_pivot.dropna()
    dti_pivot['Stage'] = dti_pivot['Stage'].str.lower()
    print(f"DTI shape after pivoting: {dti_pivot.shape}")

    # Load MRI
    mri = pd.read_parquet('mri.parquet')
    print(f"Raw MRI shape: {mri.shape}")

    # Select anatomical features
    anatomical_cols = [col for col in mri.columns if col.startswith(('lh', 'rh', 'Left', 'Right',
                       'BrainSeg', 'Cortex', 'CC', 'Cerebellum', 'Ventricle',
                       'Thalamus', 'Caudate', 'Putamen', 'Pallidum', 'Hippocampus',
                       'Amygdala', 'Accumbens'))
                       and col not in ['lhSurfaceHoles', 'rhSurfaceHoles']]

    mri_features = mri[['PATNO', 'stage_km'] + anatomical_cols].copy()
    mri_features = mri_features.rename(columns={'stage_km': 'Stage'})
    mri_features['Stage'] = mri_features['Stage'].str.lower()
    print(f"MRI features shape: {mri_features.shape}")

    # Impute
    imputer = SimpleImputer(strategy='mean')
    mri_features[anatomical_cols] = imputer.fit_transform(mri_features[anatomical_cols])
    mri_features = mri_features.dropna(subset=['PATNO', 'Stage'])
    print(f"MRI after preprocessing: {mri_features.shape}")

    # Merge
    df_imaging = dti_pivot.merge(mri_features, on=['PATNO', 'Stage'], how='inner')
    print(f"Final dataset shape: {df_imaging.shape}")
    print(f"Stage distribution:\n{df_imaging['Stage'].value_counts()}")

    if df_imaging.empty:
        raise ValueError("Merged DataFrame is empty!")

    # Separate columns
    dti_cols = [col for col in dti_pivot.columns if col not in ['PATNO', 'Stage']]
    mri_cols = anatomical_cols
    print(f"DTI features: {len(dti_cols)}, MRI features: {len(mri_cols)}")

    # Convert to numeric
    df_imaging[anatomical_cols] = df_imaging[anatomical_cols].apply(pd.to_numeric, errors='coerce')
    df_imaging[dti_cols] = df_imaging[dti_cols].apply(pd.to_numeric, errors='coerce')
    df_imaging = df_imaging.dropna(subset=anatomical_cols + dti_cols)

    # Encode labels
    le = LabelEncoder()
    df_imaging['Stage_encoded'] = le.fit_transform(df_imaging['Stage'])

    # Standardize
    scaler_dti = StandardScaler()
    scaler_mri = StandardScaler()
    df_imaging[dti_cols] = scaler_dti.fit_transform(df_imaging[dti_cols])
    df_imaging[mri_cols] = scaler_mri.fit_transform(df_imaging[mri_cols])

    # Dataset
    dataset_img = CombinedImagingDataset(df_imaging, mri_cols, dti_cols)
    patient_ids = df_imaging['PATNO'].values

    print(f"\nTotal samples: {len(dataset_img)}")

    # Load master split
    print("Loading master patient split...")
    with open('master_patient_split.pkl', 'rb') as f:
        split_data = pickle.load(f)

    train_patients = set(split_data['train_patients'])
    val_patients = set(split_data['val_patients'])
    test_patients = set(split_data['test_patients'])

    # Map indices
    train_indices = [i for i, pid in enumerate(patient_ids) if pid in train_patients]
    val_indices = [i for i, pid in enumerate(patient_ids) if pid in val_patients]
    test_indices = [i for i, pid in enumerate(patient_ids) if pid in test_patients]

    print(f"Train: {len(train_indices)}, Val: {len(val_indices)}, Test: {len(test_indices)}")

    # Verify no leakage
    train_pats = set([patient_ids[i] for i in train_indices])
    val_pats = set([patient_ids[i] for i in val_indices])
    test_pats = set([patient_ids[i] for i in test_indices])

    assert len(train_pats & val_pats) == 0, "Leakage: train-val!"
    assert len(train_pats & test_pats) == 0, "Leakage: train-test!"
    assert len(val_pats & test_pats) == 0, "Leakage: val-test!"
    print("✓ No patient leakage\n")

    # Create subsets
    train_subset = torch.utils.data.Subset(dataset_img, train_indices)
    val_subset = torch.utils.data.Subset(dataset_img, val_indices)
    test_subset = torch.utils.data.Subset(dataset_img, test_indices)

    # Data loaders
    train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_subset, batch_size=16, shuffle=False)
    test_loader = DataLoader(test_subset, batch_size=16, shuffle=False)

    # Setup model
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    n_classes = len(le.classes_)

    combined_input_dim = len(mri_cols) + len(dti_cols)
    LATENT_DIM = 16  # CRITICAL: Changed from 64 to 16

    print(f"\nCombined imaging input: {combined_input_dim}")
    print(f"  MRI: {len(mri_cols)}, DTI: {len(dti_cols)}")
    print(f"Latent dimension: {LATENT_DIM} (matches paper)")

    # Class weights
    class_counts = np.bincount(df_imaging['Stage_encoded'])
    class_weights = 1. / class_counts
    weights_tensor = torch.tensor(class_weights / class_weights.sum(), dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=weights_tensor)

    # Model
    imaging_encoder = VariationalMLPEncoder(combined_input_dim, latent_dim=LATENT_DIM).to(device)
    imaging_model = ImagingEncoderClassifier(imaging_encoder, latent_dim=LATENT_DIM, n_classes=n_classes).to(device)
    optimizer = torch.optim.Adam(imaging_model.parameters(), lr=1e-3)

    # Training
    print("\nTraining imaging encoder...")
    num_epochs = 30
    best_val_f1 = 0.0
    patience = 10
    patience_counter = 0

    for epoch in range(num_epochs):
        train_loss = train_epoch(imaging_model, train_loader, optimizer, device, criterion)
        val_loss, val_acc, val_f1, val_cm, val_preds, val_trues = evaluate(imaging_model, val_loader, device, criterion)

        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, "
                  f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}, Val F1={val_f1:.4f}")

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            patience_counter = 0
            torch.save(imaging_model.state_dict(), 'best_imaging_model.pth')
            print(f"  → Saved best model (F1: {best_val_f1:.3f})")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

    # Load best
    imaging_model.load_state_dict(torch.load('best_imaging_model.pth', weights_only=True))

    # Final evaluation
    print("\n" + "="*50)
    print("Final Validation Evaluation:")
    print("="*50)
    val_loss, val_acc, val_f1, val_cm, val_preds, val_trues = evaluate(imaging_model, val_loader, device, criterion)
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
    print(f"\nConfusion Matrix:\n{val_cm}")

    # Plot confusion matrix
    plot_confusion_matrix(val_trues, val_preds, le.classes_, title='Imaging Encoder')

    # Get embeddings for PCA
    def get_latent_embeddings(model, dataloader, device):
        model.eval()
        embeddings, labels = [], []
        with torch.no_grad():
            for x, y in dataloader:
                x = x.to(device)
                z, _, _ = model.encoder(x)
                embeddings.append(z.cpu().numpy())
                labels.extend(y.cpu().numpy())
        embeddings = np.vstack(embeddings)
        return embeddings, labels

    embeddings, labels = get_latent_embeddings(imaging_model, val_loader, device)

    # PCA visualization
    pca = PCA(n_components=2)
    reduced = pca.fit_transform(embeddings)
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(reduced[:,0], reduced[:,1], c=labels, cmap='tab10', alpha=0.7)
    plt.colorbar(scatter, ticks=[0, 1, 2], label='Stage')
    plt.title('Imaging Encoder Latent Space (PCA)')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.tight_layout()
    plt.savefig('imaging_encoder_pca.png')
    plt.show()

    # Save encoder
    torch.save(imaging_model.encoder.state_dict(), 'imaging_encoder_weights.pth')
    print("\n✓ Imaging encoder weights saved")
    print(f"✓ Output dimension: {LATENT_DIM}")
    print(f"✓ Input dimension: {combined_input_dim} (MRI: {len(mri_cols)}, DTI: {len(dti_cols)})")

