# -*- coding: utf-8 -*-
"""enrollment_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FeK09VyDhxBtt2IZAGZa1AA4TL-GPEM4
"""

import pandas as pd
import numpy as np

import pandas as pd
import os

# Define the directory path
directory = r'E:\pd-data\enrollment'

# List all CSV files in the directory
csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]
print("CSV files found in directory:", csv_files)

# Check if any CSV files were found
if not csv_files:
    raise FileNotFoundError("No CSV files found in the directory: E:\\pd-cleaned\\enr-cleaned")

# List of columns to keep (non-genetic columns)
columns_to_keep = [
    'PATNO', 'EVENT_ID', 'AGE_AT_VISIT', 'REC_ID', 'PAG_NAME', 'INFODT', 'AFICBERB', 'ASHKJEW', 'BASQUE',
    'BIRTHDT', 'SEX', 'CHLDBEAR', 'HOWLIVE', 'GAYLES', 'HETERO', 'BISEXUAL', 'PANSEXUAL', 'ASEXUAL',
    'OTHSEXUALITY', 'HANDED', 'ANYFAMPD', 'BIOMOM', 'BIOMOMPD', 'BIODAD', 'BIODADPD',
    'FULSIB', 'FULBRO', 'FULSIS', 'FULSIBPD', 'FULBROPD', 'FULSISPD', 'HAFSIB', 'PAHAFSIB', 'MAHAFSIB',
    'HAFSIBPD', 'MAHAFSIBPD', 'PAHAFSIBPD', 'MAGPAR', 'MAGPARPD', 'MAGFATHPD', 'MAGMOTHPD', 'PAGPAR',
    'PAGPARPD', 'PAGFATHPD', 'PAGMOTHPD', 'MATAU', 'MATAUPD', 'PATAU', 'PATAUPD', 'KIDSNUM', 'KIDSPD',
    'DISFAMPD', 'MATCOUS', 'MATCOUSPD', 'PATCOUS', 'PATCOUSPD', 'CLIA', 'GWAS', 'WES', 'WGS', 'SVs',
    'SANGER', 'IU_Fingerprint', 'RNASEQ', 'RNASEQ_VIS', 'APOE', 'PATHVAR_COUNT', 'VAR_GENE', 'LRRK2',
    'GBA', 'VPS35', 'SNCA', 'PRKN', 'PARK7', 'PINK1', 'NOTES', 'COHORT', 'COHORT_DEFINITION', 'ENROLL_DATE',
    'ENROLL_STATUS', 'STATUS_DATE', 'SCREENEDAM', 'ENROLL_AGE', 'INEXPAGE', 'AV133STDY', 'TAUSTDY',
    'GAITSTDY', 'PISTDY', 'SV2ASTDY', 'NXTAUSTDY', 'DATELIG', 'PPMI_ONLINE_ENROLL', 'ENRLPINK1',
    'ENRLPRKN', 'ENRLSRDC', 'ENRLNORM', 'ENRLOTHGV', 'ENRLHPSM', 'ENRLRBD', 'ENRLLRRK2', 'ENRLSNCA',
    'ENRLGBA', 'EDUCYRS', 'EducationCountry', 'EducationLevel', 'EDUCYRS_EQUIV', 'APPRDX'
]

df_merged = None
for file in csv_files:
    file_path = os.path.join(directory, file)
    print(f"Loading file: {file}")
    df = pd.read_csv(file_path)

    # Print columns in the current file for debugging
    print(f"Columns in {file}:", df.columns.tolist())

    # Check if both PATNO and EVENT_ID exist in the file
    if 'PATNO' not in df.columns or 'EVENT_ID' not in df.columns:
        print(f"Warning: 'PATNO' or 'EVENT_ID' not found in {file}. Skipping this file.")
        continue

    # If this is the first file, set it as the base DataFrame
    if df_merged is None:
        df_merged = df
    else:
        # Merge with the existing DataFrame on 'PATNO' and 'EVENT_ID' (outer join)
        df_merged = df_merged.merge(df, on=['PATNO', 'EVENT_ID'], how='outer', suffixes=('', '_dup'))

        # Handle duplicate columns (keep first non-NaN value)
        for col in df_merged.columns:
            if col.endswith('_dup'):
                original_col = col.replace('_dup', '')
                if original_col in df_merged.columns:
                    df_merged[original_col] = df_merged[original_col].combine_first(df_merged[col])
                    df_merged = df_merged.drop(columns=col)

# Check if merging was successful
if df_merged is None:
    raise ValueError("No valid DataFrames with 'PATNO' and 'EVENT_ID' were found to merge.")

# Check which columns from columns_to_keep exist in the merged DataFrame
existing_columns = [col for col in columns_to_keep if col in df_merged.columns]
missing_columns = [col for col in columns_to_keep if col not in df_merged.columns]

# Print all columns in the merged DataFrame
print("\nAll columns in merged DataFrame:", df_merged.columns.tolist())

# Print missing columns for debugging
if missing_columns:
    print("The following columns were not found in the merged DataFrame:", missing_columns)
else:
    print("All specified columns were found in the merged DataFrame.")

# Filter to keep only existing columns
df_filtered = df_merged[existing_columns]

# Further filter to keep only columns with at least one non-NaN, non-zero value
non_null_non_zero_columns = [
    col for col in df_filtered.columns
    if df_filtered[col].notna().any() and (df_filtered[col][df_filtered[col].notna()] != 0).any()
]
df_filtered = df_filtered[non_null_non_zero_columns]

# Print columns that were removed due to containing only NaN or only zero values
excluded_columns = [col for col in existing_columns if col not in non_null_non_zero_columns]
if excluded_columns:
    print("The following columns were removed because they contain only NaN or only zero values:", excluded_columns)
else:
    print("No columns were removed for containing only NaN or only zero values.")

# Save the filtered DataFrame to the specified directory
output_file = os.path.join(directory, 'filterdata.csv')
df_filtered.to_csv(output_file, index=False)
print(f"\nFiltered dataset saved to: {output_file}")

# Display the first few rows of the filtered DataFrame
print("\nFiltered DataFrame (first 5 rows):")
print(df_filtered.head())

# Display the columns in the filtered dataset
print("\nColumns in filtered dataset:", df_filtered.columns.tolist())

df_filtered.head(30)

directory = r'E:\pd-cleaned\enr-cleaned'
output_file = os.path.join(directory, 'filterdata.csv')

# Save the filtered DataFrame
df_filtered.to_csv(output_file, index=False)
print(f"Filtered dataset saved to: {output_file}")

df = pd.read_csv(r'E:\pd-cleaned\enr-cleaned\filterdata.csv')

columns_to_remove = ['BIRTHDT', 'HOWLIVE', 'GAYLES', 'HETERO', 'BISEXUAL', 'ASEXUAL', 'OTHSEXUALITY', 'INEXPAGE', 'EducationCountry', 'EducationLevel', 'EDUCYRS_EQUIV', 'APPRDX']

# Remove specified columns from df_filtered
df_filtered = df_filtered.drop(columns=[col for col in columns_to_remove if col in df_filtered.columns])

# Define the directory and output file path
directory = r'E:\pd-cleaned\enr-cleaned'
output_file = os.path.join(directory, 'filterdata.csv')

# Save the filtered DataFrame
df_filtered.to_csv(output_file, index=False)
print(f"Filtered dataset saved to: {output_file}")

df.head(5)

df.columns

import matplotlib.pyplot as plt
import numpy as np

# Histogram
fig, ax = plt.subplots(figsize=(10, 6))
n, bins, patches = ax.hist(df['ENROLL_AGE'].dropna(), bins=20, color='skyblue', edgecolor='black')

# Mean and median
mean_age = df['ENROLL_AGE'].mean()
median_age = df['ENROLL_AGE'].median()

# Annotate mean and median
ax.axvline(mean_age, color='red', linestyle='dashed', linewidth=1.5, label=f'Mean: {mean_age:.1f}')
ax.axvline(median_age, color='green', linestyle='dotted', linewidth=1.5, label=f'Median: {median_age:.1f}')

# Improved labels and title
ax.set_xlabel('Age at Enrollment')
ax.set_ylabel('Patient Count')
ax.set_title('Distribution of Age at Enrollment')
ax.legend()

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Filter out cohort 3
df_filtered = df[df['COHORT'] != 3]

# Compute proportions within each sex after filtering
ct = pd.crosstab(df_filtered['SEX'], df_filtered['COHORT'], normalize='index') * 100

# Plot with stacked bar chart and clear title for cohorts
ct.plot(kind='bar', stacked=True, colormap='Paired')
plt.title('Percentage Distribution of Cohort by Sex\n(1: PD, 2: Healthy, 4: Prodromal)')
plt.ylabel('Percentage (%)')
plt.xlabel('Sex (0=Female, 1=Male)')
plt.legend(title='Cohort')
plt.tight_layout()
plt.show()

# Summary table without cohort 3
summary = pd.crosstab(df_filtered['SEX'], df_filtered['COHORT'])
print(summary)

# Count males and females
counts = df['SEX'].value_counts()
print(counts)

cohort_counts = df['COHORT'].value_counts()
print(cohort_counts)

# View unique cohort definitions
unique_defs = df['COHORT_DEFINITION'].unique()
print(unique_defs)

# Or view value counts of each definition
value_counts = df['COHORT_DEFINITION'].value_counts()
print(value_counts)

df_filtered['PATNO'] = df_filtered['PATNO'].astype(str)
df_filtered.to_csv('enrollment_df.csv', index=False)

print("Columns in enrollment_df:", df_filtered.columns.tolist())

df_filtered.head(200)

# Sort and group data
df_sorted = df_filtered.sort_values(['PATNO', 'EVENT_ID'])

# Select key variables to track condition (adjust as needed)
condition_vars = ['APPRDX', 'GAITSTDY', 'PISTDY', 'TAUSTDY']

# Example: Change in diagnosis over visits for each patient
for patno, group in df_sorted.groupby('PATNO'):
    print(f"Patient {patno}")
    print(group[['EVENT_ID', 'APPRDX']])
    print("------")

# Plotting example for a clinical variable over visits
import matplotlib.pyplot as plt

for patno, group in df_sorted.groupby('PATNO'):
    plt.plot(group['EVENT_ID'], group['GAITSTDY'], marker='o', alpha=0.3)

plt.xlabel('Visit')
plt.ylabel('Gait Study Day or Score')
plt.title('Clinical Variable Trend over Visits')
plt.show()

