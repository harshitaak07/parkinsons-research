# -*- coding: utf-8 -*-
"""complete_training_pipeline (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cV0-KNXgJ7PodOT6wKutqhS6SyR_xwKL
"""

"""
Fixed Training Pipeline - Handles Small Batches with BatchNorm
Fixes:
1. Uses drop_last=True for training to avoid single-sample batches
2. Uses model.eval() mode for validation (disables BatchNorm training mode)
3. Reduces batch size to 8 for better handling of small datasets
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                             roc_auc_score, confusion_matrix)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# CLASSIFIER AND TRAINING COMPONENTS
# ============================================================================

class PDStageClassifier(nn.Module):
    """Three-stage Parkinson's Disease classifier"""
    def __init__(self, input_dim=192, hidden_dim=128, num_classes=3, dropout=0.4):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


class CompletePDModel(nn.Module):
    """Complete model: Fused Encoder + Classifier"""
    def __init__(self, fused_encoder, classifier):
        super().__init__()
        self.fused_encoder = fused_encoder
        self.classifier = classifier

    def forward(self, motor_x, nonmotor_x, imaging_x):
        fused_emb, individual_embs = self.fused_encoder(motor_x, nonmotor_x, imaging_x)
        logits = self.classifier(fused_emb)
        return logits, fused_emb, individual_embs


def compute_class_weights(labels, num_classes=3):
    """Compute weights for weighted cross-entropy loss"""
    counts = np.bincount(labels, minlength=num_classes)
    weights = len(labels) / (num_classes * counts)
    return torch.FloatTensor(weights)


def train_epoch(model, dataloader, criterion, optimizer, device, patient_to_label):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []

    for batch in tqdm(dataloader, desc="Training", leave=False):
        motor_x, nonmotor_x, imaging_x, patient_ids = batch

        # Get labels from patient IDs
        labels = torch.tensor([patient_to_label[str(pid)] for pid in patient_ids],
                              dtype=torch.long)

        motor_x = motor_x.to(device)
        nonmotor_x = tuple(x.to(device) for x in nonmotor_x)
        imaging_x = imaging_x.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        logits, fused_emb, individual_embs = model(motor_x, nonmotor_x, imaging_x)
        loss = criterion(logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds)


def validate_epoch(model, dataloader, criterion, device, patient_to_label):
    """Validate for one epoch - uses eval mode to handle BatchNorm"""
    model.eval()  # CRITICAL: Set to eval mode
    total_loss = 0
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validating", leave=False):
            motor_x, nonmotor_x, imaging_x, patient_ids = batch

            # Get labels from patient IDs
            labels = torch.tensor([patient_to_label[str(pid)] for pid in patient_ids],
                                  dtype=torch.long)

            motor_x = motor_x.to(device)
            nonmotor_x = tuple(x.to(device) for x in nonmotor_x)
            imaging_x = imaging_x.to(device)
            labels = labels.to(device)

            logits, _, _ = model(motor_x, nonmotor_x, imaging_x)
            loss = criterion(logits, labels)
            total_loss += loss.item()

            probs = F.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)

    return avg_loss, accuracy, all_preds, all_labels, np.array(all_probs)


def train_with_early_stopping(model, train_loader, val_loader, criterion,
                              optimizer, device, patient_to_label, num_epochs=100,
                              patience=10, save_path='best_model.pth'):
    """Train with early stopping"""
    best_val_acc = 0
    patience_counter = 0
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    print("\nStarting training...")

    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer,
                                           device, patient_to_label)
        val_loss, val_acc, _, _, _ = validate_epoch(model, val_loader, criterion,
                                                     device, patient_to_label)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        print(f"Epoch {epoch+1:3d}/{num_epochs} | "
              f"Train Loss: {train_loss:.4f} Acc: {train_acc*100:5.1f}% | "
              f"Val Loss: {val_loss:.4f} Acc: {val_acc*100:5.1f}%")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss
            }, save_path)
            print(f"  ✓ Best model saved (Val Acc: {val_acc*100:.1f}%)")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    checkpoint = torch.load(save_path, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])

    return model, train_losses, val_losses, train_accs, val_accs


def compute_metrics(y_true, y_pred, y_probs, stage_names=['Early', 'Mid', 'Late']):
    """Compute comprehensive metrics"""
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average=None, zero_division=0
    )
    macro_f1 = np.mean(f1)

    try:
        auc = roc_auc_score(y_true, y_probs, multi_class='ovr', average='macro')
    except:
        auc = 0.0

    per_class_metrics = {}
    for i, stage in enumerate(stage_names):
        per_class_metrics[stage] = {
            'precision': precision[i],
            'recall': recall[i],
            'f1': f1[i],
            'support': support[i]
        }

    cm = confusion_matrix(y_true, y_pred)

    return {
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'auc': auc,
        'per_class': per_class_metrics,
        'confusion_matrix': cm
    }


def print_metrics(metrics, fold=None):
    """Pretty print metrics"""
    prefix = f"Fold {fold} - " if fold is not None else ""

    print(f"\n{prefix}Overall Metrics:")
    print(f"  Accuracy: {metrics['accuracy']*100:.1f}%")
    print(f"  Macro F1: {metrics['macro_f1']*100:.1f}%")
    print(f"  AUC: {metrics['auc']:.3f}")

    print(f"\n{prefix}Per-Stage Metrics:")
    for stage, vals in metrics['per_class'].items():
        print(f"  {stage}:")
        print(f"    Precision: {vals['precision']*100:.1f}%")
        print(f"    Recall: {vals['recall']*100:.1f}%")
        print(f"    F1-Score: {vals['f1']*100:.1f}%")
        print(f"    Support: {vals['support']}")


# ============================================================================
# LABEL EXTRACTION
# ============================================================================

def create_patient_to_label_map(dataset):
    """Create mapping from patient ID to label using motor_df"""
    print("\nCreating patient ID to label mapping...")

    # Get common patients (those in the dataset)
    common_patients = dataset.common_patients

    # Create mapping
    patient_to_label = {}
    motor_df = dataset.motor_df

    for patient_id in common_patients:
        # Find label in motor_df
        patient_rows = motor_df[motor_df['PATNO'] == patient_id]
        if len(patient_rows) > 0:
            label = int(patient_rows['stage_label'].iloc[0])
            patient_to_label[str(patient_id)] = label

    print(f"Created mapping for {len(patient_to_label)} patients")

    # Print label distribution
    labels = list(patient_to_label.values())
    print("\nLabel distribution:")
    for label in [0, 1, 2]:
        count = labels.count(label)
        print(f"  Class {label}: {count} samples ({count/len(labels)*100:.1f}%)")

    return patient_to_label, np.array(labels)


# ============================================================================
# CROSS-VALIDATION
# ============================================================================

def cross_validate(dataset, fused_encoder, device, patient_to_label, all_labels,
                   n_splits=5, num_epochs=100, batch_size=8, lr=1e-3):
    """5-fold stratified cross-validation with proper batch handling"""
    print("\n" + "="*70)
    print("5-FOLD STRATIFIED CROSS-VALIDATION")
    print("="*70)

    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_results = []
    all_cms = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_labels)), all_labels)):
        print(f"\n{'='*70}")
        print(f"FOLD {fold+1}/{n_splits}")
        print(f"{'='*70}")
        print(f"Train: {len(train_idx)} samples | Val: {len(val_idx)} samples")

        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)

        # CRITICAL: Use drop_last=True for training to avoid single-sample batches
        train_loader = DataLoader(train_subset, batch_size=batch_size,
                                 shuffle=True, num_workers=0, drop_last=True)
        # Validation uses shuffle=False and drop_last=False since we use eval() mode
        val_loader = DataLoader(val_subset, batch_size=batch_size,
                               shuffle=False, num_workers=0, drop_last=False)

        # Compute class weights
        train_labels = all_labels[train_idx]
        class_weights = compute_class_weights(train_labels)
        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))

        # Create model
        classifier = PDStageClassifier(
            input_dim=fused_encoder.get_embedding_dim(),
            hidden_dim=128,
            num_classes=3,
            dropout=0.4
        )
        model = CompletePDModel(fused_encoder, classifier).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)

        # Train
        model, train_losses, val_losses, train_accs, val_accs = train_with_early_stopping(
            model, train_loader, val_loader, criterion, optimizer, device,
            patient_to_label, num_epochs=num_epochs, patience=10,
            save_path=f'best_model_fold{fold+1}.pth'
        )

        # Final evaluation
        _, val_acc, y_pred, y_true, y_probs = validate_epoch(model, val_loader, criterion,
                                                              device, patient_to_label)
        metrics = compute_metrics(y_true, y_pred, y_probs)
        print_metrics(metrics, fold=fold+1)

        fold_results.append({
            'fold': fold+1,
            'accuracy': metrics['accuracy'],
            'macro_f1': metrics['macro_f1'],
            'auc': metrics['auc'],
            'per_class': metrics['per_class'],
            'train_losses': train_losses,
            'val_losses': val_losses,
            'train_accs': train_accs,
            'val_accs': val_accs
        })
        all_cms.append(metrics['confusion_matrix'])

    # Aggregate results
    accuracies = [r['accuracy'] for r in fold_results]
    f1_scores = [r['macro_f1'] for r in fold_results]
    aucs = [r['auc'] for r in fold_results]

    print(f"\n{'='*70}")
    print("CROSS-VALIDATION SUMMARY")
    print(f"{'='*70}")
    print(f"Accuracy: {np.mean(accuracies)*100:.1f}% ± {np.std(accuracies)*100:.1f}%")
    print(f"Macro F1: {np.mean(f1_scores)*100:.1f}% ± {np.std(f1_scores)*100:.1f}%")
    print(f"AUC: {np.mean(aucs):.3f} ± {np.std(aucs):.3f}")

    return fold_results, all_cms


# ============================================================================
# VISUALIZATION
# ============================================================================

def plot_confusion_matrix(cm, stage_names=['Early', 'Mid', 'Late'],
                         title='Confusion Matrix', save_path=None):
    """Plot confusion matrix as percentages"""
    cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues',
                xticklabels=stage_names, yticklabels=stage_names,
                cbar_kws={'label': 'Percentage (%)'})
    plt.title(title, fontsize=14, fontweight='bold')
    plt.ylabel('True Stage', fontsize=12)
    plt.xlabel('Predicted Stage', fontsize=12)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


def plot_training_curves(fold_results, save_path=None):
    """Plot training vs validation curves"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for i, result in enumerate(fold_results):
        epochs = range(1, len(result['train_losses']) + 1)
        axes[0].plot(epochs, result['train_losses'], label=f'Fold {i+1} Train', alpha=0.6)
        axes[0].plot(epochs, result['val_losses'], label=f'Fold {i+1} Val', alpha=0.6, linestyle='--')

    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=8, ncol=2)
    axes[0].grid(True, alpha=0.3)

    for i, result in enumerate(fold_results):
        epochs = range(1, len(result['train_accs']) + 1)
        axes[1].plot(epochs, result['train_accs'], label=f'Fold {i+1} Train', alpha=0.6)
        axes[1].plot(epochs, result['val_accs'], label=f'Fold {i+1} Val', alpha=0.6, linestyle='--')

    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=8, ncol=2)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


# ============================================================================
# MAIN EXECUTION
# ============================================================================

print("\n" + "="*70)
print("PARKINSON'S DISEASE COMPLETE TRAINING PIPELINE")
print("="*70)

# Create patient-to-label mapping
patient_to_label, all_labels = create_patient_to_label_map(dataset)

# Train/test split
indices = np.arange(len(dataset))
train_idx, test_idx = train_test_split(
    indices, test_size=0.2, stratify=all_labels, random_state=42
)

print(f"\nTrain samples: {len(train_idx)}")
print(f"Test samples: {len(test_idx)}")

# Cross-validation on training set
print("\n" + "="*70)
print("STARTING CROSS-VALIDATION")
print("="*70)

train_dataset = torch.utils.data.Subset(dataset, train_idx)
train_labels = all_labels[train_idx]

fold_results, confusion_matrices = cross_validate(
    dataset=train_dataset,
    fused_encoder=fused_encoder,
    device=DEVICE,
    patient_to_label=patient_to_label,
    all_labels=train_labels,
    n_splits=5,
    num_epochs=100,
    batch_size=8,  # Reduced batch size for small dataset
    lr=1e-3
)

# External validation on test set
print("\n" + "="*70)
print("EXTERNAL VALIDATION (HELD-OUT TEST SET)")
print("="*70)

test_dataset = torch.utils.data.Subset(dataset, test_idx)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Load best model
checkpoint = torch.load('best_model_fold1.pth', weights_only=False)
classifier = PDStageClassifier(
    input_dim=fused_encoder.get_embedding_dim(),
    hidden_dim=128,
    num_classes=3,
    dropout=0.4
)
model = CompletePDModel(fused_encoder, classifier).to(DEVICE)
model.load_state_dict(checkpoint['model_state_dict'])

# Evaluate
model.eval()
all_preds = []
all_labels_test = []
all_probs = []

with torch.no_grad():
    for batch in test_loader:
        motor_x, nonmotor_x, imaging_x, patient_ids = batch

        # Get labels from patient IDs
        labels = torch.tensor([patient_to_label[str(pid)] for pid in patient_ids],
                              dtype=torch.long)

        motor_x = motor_x.to(DEVICE)
        nonmotor_x = tuple(x.to(DEVICE) for x in nonmotor_x)
        imaging_x = imaging_x.to(DEVICE)

        logits, _, _ = model(motor_x, nonmotor_x, imaging_x)
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels_test.extend(labels.cpu().numpy())
        all_probs.extend(probs.cpu().numpy())

test_metrics = compute_metrics(
    np.array(all_labels_test),
    np.array(all_preds),
    np.array(all_probs)
)

print("\nExternal Validation Results:")
print(f"  Accuracy: {test_metrics['accuracy']*100:.1f}%")
print(f"  Macro F1: {test_metrics['macro_f1']*100:.1f}%")
print(f"  AUC: {test_metrics['auc']:.3f}")
print_metrics(test_metrics)

# Visualizations
print("\n" + "="*70)
print("GENERATING VISUALIZATIONS")
print("="*70)

avg_cm = np.mean(confusion_matrices, axis=0)
plot_confusion_matrix(avg_cm, title='Cross-Validation Confusion Matrix (%)',
                     save_path='confusion_matrix_cv.png')

plot_confusion_matrix(test_metrics['confusion_matrix'],
                     title='External Validation Confusion Matrix (%)',
                     save_path='confusion_matrix_test.png')

plot_training_curves(fold_results, save_path='training_curves.png')

print("\n✓ Training complete!")
print("✓ All results saved!")

