# -*- coding: utf-8 -*-
"""main_exec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nY-nxR7H-WBb3BPq9veOcLEofjS6Y8lw
"""

"""
COMPLETE MAIN EXECUTION SCRIPT FOR PARKINSON'S DISEASE CLASSIFICATION
Fixed version with dimension matching for encoder weights

Prerequisites:
1. All parquet files in the same directory (motor.parquet, cog.parquet, merged_df.parquet,
   df2_Selected.parquet, Merged_Df.parquet, sleep.parquet, dti.parquet, mri.parquet)
2. master_patient_split.pkl (run master_patient_split_(1).py first)
3. Trained encoder weights (motor_encoder_weights.pth, nonmotor_encoder_weights.pth,
   imaging_encoder_weights.pth)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import pickle
from sklearn.metrics import (accuracy_score, precision_recall_fscore_support,
                            roc_auc_score, confusion_matrix, f1_score)
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import wilcoxon
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

import sys
import datetime

def print_inference_report(patno, visit, diagnosis, confidence, modality_weights, key_features):
    """Pretty console output for real model inference."""

    # Date/time
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    sys.stdout.write(f"$ python multimodal_pd_predictor.py --patno {patno} --visit {visit}\n\n")
    sys.stdout.write("====================================================================\n")
    sys.stdout.write("SEQUENTIAL MULTIMODAL PD RISK ASSESSMENT FRAMEWORK\n")
    sys.stdout.write(f"(Inference Run: {now} IST)\n")
    sys.stdout.write("====================================================================\n\n")

    sys.stdout.write(f"--- FINAL PREDICTION ---\n")
    sys.stdout.write(f"DIAGNOSIS:               {diagnosis}\n")
    sys.stdout.write(f"PREDICTION CONFIDENCE:   {confidence:.1f}%\n")
    sys.stdout.write("--------------------------------------------------------------------\n\n")

    sys.stdout.write("--- INTERPRETABILITY: MODALITY CONTRIBUTION (ATTENTION WEIGHTS) ---\n")
    sys.stdout.write("(Explaining *why* the prediction was made: Total must sum to 100%)\n")
    sys.stdout.write("| Modality | Contribution (%) | Specific Feature Highlight |\n")
    sys.stdout.write("|----------|------------------|----------------------------|\n")

    for (modality, weight), highlight in zip(modality_weights, key_features):
        bar = f"{weight*100:5.1f}%"
        sys.stdout.write(f"| {modality:<8} | {bar:<16} | {highlight:<26} |\n")

    sys.stdout.write("--------------------------------------------------------------------\n\n")


try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("Warning: SHAP not available. Install with: pip install shap")


# ============================================================================
# ENCODER ARCHITECTURES
# ============================================================================

class TransformerMotorEncoder(nn.Module):
    """Transformer-based Motor Encoder - 16-dim output"""
    def __init__(self, input_dim=14, latent_dim=16, num_heads=4, num_layers=2, dropout=0.4):
        super().__init__()
        self.embedding = nn.Sequential(
            nn.Linear(input_dim, latent_dim),
            nn.LayerNorm(latent_dim),
            nn.Dropout(dropout)
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=latent_dim, nhead=num_heads, dim_feedforward=latent_dim*4,
            dropout=dropout, activation='gelu', batch_first=True, norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output_norm = nn.LayerNorm(latent_dim)

    def forward(self, x):
        # x is expected to be (batch, input_dim). unsqueeze(1) makes it (batch, 1, input_dim) for transformer sequence of length 1
        x = self.embedding(x.unsqueeze(1))
        x = self.transformer(x)
        x = x.squeeze(1)
        return self.output_norm(x)


class CategoryEncoder(nn.Module):
    """LSTM encoder for non-motor categories (sequence data)"""
    def __init__(self, input_dim, embed_dim):
        super().__init__()
        hidden = embed_dim // 2
        # Input: (batch, seq_len, input_dim) -> Output: (batch, seq_len, hidden*2)
        self.lstm = nn.LSTM(input_dim, hidden, bidirectional=True, batch_first=True)
        # Assuming the final output is based on the sequence mean/last time step,
        # but the original code structure passes the sequence output (h) to mu/logvar
        # We'll use sequence output shape (batch, seq_len, embed_dim) where embed_dim = hidden*2
        self.mu = nn.Linear(embed_dim, embed_dim)
        self.logvar = nn.Linear(embed_dim, embed_dim)

    def reparam(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            return mu + std * torch.randn_like(std)
        return mu

    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        h, _ = self.lstm(x) # h: (batch, seq_len, embed_dim)
        mu, logvar = self.mu(h), self.logvar(h)
        return self.reparam(mu, logvar), mu, logvar


class NonMotorEncoder(nn.Module):
    """Non-Motor Encoder - 16-dim common output (fused via mean pooling)"""
    def __init__(self, input_dims, embed_dims, common_dim=16):
        super().__init__()
        self.encoders = nn.ModuleList([CategoryEncoder(dim, ed) for dim, ed in zip(input_dims, embed_dims)])
        # Project from each category's embed_dim to the common_dim (16)
        self.projectors = nn.ModuleList([nn.Linear(ed, common_dim) for ed in embed_dims])
        self.common_dim = common_dim

    def forward(self, x_list):
        zs, mus, logs = [], [], []
        for encoder, proj, x in zip(self.encoders, self.projectors, x_list):
            # x: (batch, seq_len, input_dim)
            z_seq, mu, logvar = encoder(x) # z_seq: (batch, seq_len, embed_dim)

            # Project each time step's embedding to common_dim
            z_proj_seq = proj(z_seq) # z_proj_seq: (batch, seq_len, common_dim)

            # Sequence pooling (mean over sequence length)
            z_pooled = z_proj_seq.mean(dim=1) # z_pooled: (batch, common_dim)

            zs.append(z_pooled)
            mus.append(mu)
            logs.append(logvar)

        # Fused is the mean of the pooled embeddings across the 5 non-motor categories
        # Stacked: (batch, common_dim, num_categories) -> Mean across categories: (batch, common_dim)
        fused = torch.stack(zs, dim=2).mean(dim=2)

        # Note: The original code had an extra .mean(dim=1) which is likely incorrect
        # if zs already holds pooled (batch, common_dim) features.
        # However, to maintain the original intent, we'll ensure 'pooled' is the intended single feature vector.
        # Since zs already contains pooled features (batch, common_dim), 'fused' is the final output.
        pooled = fused # final nonmotor embedding (batch, common_dim)

        return pooled, mus, logs


class ImagingEncoder(nn.Module):
    """Imaging Encoder - 16-dim output (VAE-like structure)"""
    def __init__(self, input_dim, latent_dim=16, hidden_dims=[256, 128], dropout_p=0.4):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for h_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, h_dim))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(h_dim))
            layers.append(nn.Dropout(dropout_p))
            prev_dim = h_dim
        self.encoder = nn.Sequential(*layers)
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)
        self.latent_dim = latent_dim

    def reparameterize(self, mu, logvar):
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        return mu

    def forward(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar


class AttentionFusion(nn.Module):
    """Attention-based fusion - outputs 48-dim (16 x 3)"""
    def __init__(self, latent_dim=16, num_modalities=3, num_heads=4):
        super().__init__()
        self.latent_dim = latent_dim
        self.num_modalities = num_modalities
        # MultiheadAttention requires K, Q, V of shape (sequence_length, batch_size, embed_dim)
        # Since we have 3 "tokens" (modalities) per patient, sequence_length=3
        self.attention = nn.MultiheadAttention(
            embed_dim=latent_dim, num_heads=num_heads, batch_first=True, dropout=0.1
        )
        self.norm = nn.LayerNorm(latent_dim)
        self.fused_dim = latent_dim * num_modalities

    def forward(self, motor_emb, nonmotor_emb, imaging_emb):
        # motor/nonmotor/imaging_emb are (batch_size, latent_dim)
        # Stacked: (batch_size, num_modalities, latent_dim)
        stacked = torch.stack([motor_emb, nonmotor_emb, imaging_emb], dim=1)

        # Q, K, V are all the stacked embeddings
        # attended: (batch_size, num_modalities, latent_dim)
        attended, attn_weights = self.attention(stacked, stacked, stacked)

        attended = self.norm(attended)
        # Flatten to (batch_size, num_modalities * latent_dim) = (batch_size, 48)
        fused = attended.reshape(attended.size(0), -1)
        return fused, attn_weights


class UnifiedFusedEncoder(nn.Module):
    """Complete fused encoder with attention - 48-dim output"""
    def __init__(self, motor_encoder, nonmotor_encoder, imaging_encoder, latent_dim=16, freeze_encoders=True):
        super().__init__()
        self.motor_encoder = motor_encoder
        self.nonmotor_encoder = nonmotor_encoder
        self.imaging_encoder = imaging_encoder
        self.latent_dim = latent_dim
        self.freeze_encoders = freeze_encoders

        if freeze_encoders:
            for param in self.motor_encoder.parameters():
                param.requires_grad = False
            for param in self.nonmotor_encoder.parameters():
                param.requires_grad = False
            for param in self.imaging_encoder.parameters():
                param.requires_grad = False

        self.fusion = AttentionFusion(latent_dim=latent_dim, num_modalities=3)
        self.fused_dim = 48

    def forward(self, motor_x, nonmotor_x_list, imaging_x):
        # Get embeddings: all are (batch_size, latent_dim)
        motor_emb = self.motor_encoder(motor_x)
        nonmotor_emb, mus, logs = self.nonmotor_encoder(nonmotor_x_list)
        imaging_emb, img_mu, img_logvar = self.imaging_encoder(imaging_x)

        # Fuse embeddings
        fused, attn_weights = self.fusion(motor_emb, nonmotor_emb, imaging_emb)

        individual_embeddings = {
            'motor': motor_emb,
            'nonmotor': nonmotor_emb,
            'imaging': imaging_emb,
            'attention_weights': attn_weights
        }
        return fused, individual_embeddings

    def get_embedding_dim(self):
        return self.fused_dim


class PDStageClassifier(nn.Module):
    """Three-stage Parkinson's Disease classifier"""
    def __init__(self, input_dim=48, hidden_dim=128, num_classes=3, dropout=0.4):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


class CompletePDModel(nn.Module):
    """Complete model: Fused Encoder + Classifier"""
    def __init__(self, fused_encoder, classifier):
        super().__init__()
        self.fused_encoder = fused_encoder
        self.classifier = classifier

    def forward(self, motor_x, nonmotor_x, imaging_x):
        fused_emb, individual_embs = self.fused_encoder(motor_x, nonmotor_x, imaging_x)
        logits = self.classifier(fused_emb)
        return logits, fused_emb, individual_embs


# ============================================================================
# BASELINE MODELS (Included for completeness, not used in main execution)
# ============================================================================

class LSTMLateBaseline(nn.Module):
    """LSTM + Late Fusion baseline"""
    def __init__(self, motor_dim=14, nonmotor_embed_dim=32, imaging_dim=147):
        super().__init__()
        # Motor: (batch, 1, 14) -> (batch, 1, 32) -> (batch, 32)
        self.motor_lstm = nn.LSTM(motor_dim, 32, batch_first=True)
        # Non-motor: The original script used nonmotor_x[0] which is the first sequence category
        # Assuming the first category is ~101 dim and embed_dim is 32*2=64 in the original design,
        # but here we use a simplified late fusion for the placeholder.
        self.nonmotor_lstm = nn.LSTM(101, 32, batch_first=True)
        self.imaging_fc = nn.Linear(imaging_dim, 32)
        self.classifier = nn.Linear(32 * 3, 3) # 32 motor + 32 non-motor + 32 imaging = 96

    def forward(self, motor_x, nonmotor_x, imaging_x):
        # Motor: (batch, 14) -> (batch, 1, 14) for LSTM
        motor_out, _ = self.motor_lstm(motor_x.unsqueeze(1))
        motor_feat = motor_out[:, -1, :] # Last time step output

        # Non-motor: Use the first non-motor sequence (x_list[0])
        nm_out, _ = self.nonmotor_lstm(nonmotor_x[0])
        nm_feat = nm_out.mean(dim=1) # Mean pooling over sequence length

        # Imaging
        img_feat = self.imaging_fc(imaging_x)

        fused = torch.cat([motor_feat, nm_feat, img_feat], dim=1) # (batch, 96)
        return self.classifier(fused)


# ============================================================================
# DATASET CLASS
# ============================================================================

class MultimodalParkinsonsDataset(Dataset):
    """Complete multimodal dataset with proper preprocessing"""
    def __init__(self, motor_df, nonmotor_seqs, imaging_df, patient_ids, labels_df):
        self.motor_df = motor_df
        # nonmotor_seqs is a list of tuples, where each tuple contains 5 non-motor sequence tensors
        self.nonmotor_seqs = nonmotor_seqs
        self.imaging_df = imaging_df
        self.patient_ids = list(patient_ids)
        self.labels_df = labels_df
        self.label_map = dict(zip(labels_df['PATNO'], labels_df['label']))
        self.motor_cols = None # To be set by load_and_preprocess_all_data
        self.imaging_cols = None # To be set by load_and_preprocess_all_data
        print(f"✓ Dataset initialized: {len(self.patient_ids)} patients")

    def __len__(self):
        return len(self.patient_ids)

    def __getitem__(self, idx):
        patient_id = self.patient_ids[idx]

        # Motor features (static/visit-specific)
        motor_row = self.motor_df[self.motor_df['PATNO'] == patient_id].iloc[0]
        motor_values = motor_row[self.motor_cols].values.astype(np.float32)
        motor_features = torch.tensor(motor_values, dtype=torch.float32)

        # Non-motor sequences (tuple of 5 tensors)
        nonmotor_tuple = self.nonmotor_seqs[idx] # Already a tuple of tensors

        # Imaging features (static/visit-specific)
        imaging_row = self.imaging_df[self.imaging_df['PATNO'] == patient_id].iloc[0]
        imaging_values = imaging_row[self.imaging_cols].values.astype(np.float32)
        imaging_features = torch.tensor(imaging_values, dtype=torch.float32)

        label = self.label_map.get(patient_id, 0)

        # The NonMotorEncoder expects a tuple of tensors, so we return the tuple directly
        return motor_features, nonmotor_tuple, imaging_features, patient_id


# ============================================================================
# HELPER FUNCTION TO LOAD ENCODER WITH DIMENSION CHECK
# ============================================================================

def load_encoder_with_check(encoder, weight_path, device):
    """Load encoder weights with dimension mismatch handling"""
    try:
        # Try to load directly
        # Must ensure the weights are loaded to the correct device
        state_dict = torch.load(weight_path, map_location=device)

        # This check is crucial for the Transformer Motor Encoder if it was trained on a different version
        # It attempts to fix common state_dict key discrepancies if the model definition changed slightly
        new_state_dict = {}
        for k, v in state_dict.items():
            # Handle potential mismatch in key names from older PyTorch versions
            if k.startswith('module.'):
                k = k[7:]  # remove 'module.' prefix if it exists
            new_state_dict[k] = v

        encoder.load_state_dict(new_state_dict, strict=True)
        print(f"✓ Loaded weights from {weight_path}")
        return True
    except (RuntimeError, FileNotFoundError) as e:
        if isinstance(e, FileNotFoundError) or "size mismatch" in str(e) or "Missing key(s)" in str(e) or "Unexpected key(s)" in str(e):
            print(f"⚠ Warning: Weights file not found or dimension/key mismatch detected in {weight_path}")
            if isinstance(e, FileNotFoundError):
                print(f"  File does not exist: {weight_path}")
            else:
                print(f"  {str(e)[:200]}...")
            # Initialize with random weights
            print(f"  Initializing encoder with random weights instead")
            return False
        else:
            raise e


# ============================================================================
# DATA LOADING AND PREPROCESSING
# ============================================================================
def load_and_preprocess_all_data():
    """Load all datasets and prepare them"""
    print("\n" + "="*70)
    print("LOADING AND PREPROCESSING DATA")
    print("="*70)

    # Load motor data
    print("\n1. Loading motor data...")
    df_motor = pd.read_parquet('motor.parquet', engine='pyarrow')
    motor_cols = ['NP2SPCH', 'NP2SALV', 'NP2SWAL', 'NP2EAT', 'NP2DRES', 'NP2HYGN',
                  'NP2HWRT', 'NP2HOBB', 'NP2TURN', 'NP2TRMR', 'NP2RISE', 'NP2WALK',
                  'NP2FREZ', 'Total_Motor_Score']

    motor_patients = set(df_motor['PATNO'].unique())

    # Ensure all motor columns are numeric
    for col in motor_cols:
        df_motor[col] = pd.to_numeric(df_motor[col], errors='coerce')

    imputer_motor = SimpleImputer(strategy='mean')
    df_motor[motor_cols] = imputer_motor.fit_transform(df_motor[motor_cols])
    scaler_motor = StandardScaler()
    df_motor[motor_cols] = scaler_motor.fit_transform(df_motor[motor_cols])

    df_motor[motor_cols] = df_motor[motor_cols].astype(np.float32)

    # Select the latest visit for static features
    df_motor = df_motor.sort_values(['PATNO', 'EVENT_ID']).groupby('PATNO').tail(1).reset_index(drop=True)

    print(f"   ✓ Motor: {len(df_motor)} patients, {len(motor_cols)} features")

    # Load non-motor data
    print("\n2. Loading non-motor data...")
    df_cog = pd.read_parquet('cog.parquet', engine='pyarrow')
    df_psych = pd.read_parquet('merged_df.parquet', engine='pyarrow')
    df_sleep = pd.read_parquet('df2_Selected.parquet', engine='pyarrow')
    df_auto = pd.read_parquet('Merged_Df.parquet', engine='pyarrow')
    df_sens = pd.read_parquet('sleep.parquet', engine='pyarrow')

    # Non-numeric columns to be ignored (but NOT PATNO and EVENT_ID for grouping)
    non_numeric_cols = ['REC_ID_x', 'PAG_NAME_x', 'INFODT_x', 'BIRTHDT', 'CLIA', 'GWAS',
                       'WES', 'WGS', 'SVs', 'SANGER', 'IU_Fingerprint', 'RNASEQ', 'APOE',
                       'REC_ID', 'INFODT', 'PAG_NAME']

    def preprocess_keep_numeric(df, drop_cols):
        """Preprocess sequences while keeping PATNO and EVENT_ID for grouping"""
        df = df.sort_values(['PATNO', 'EVENT_ID'])

        # Drop non-numeric columns (but keep PATNO and EVENT_ID)
        cols_to_drop = [c for c in drop_cols if c in df.columns and c not in ['PATNO', 'EVENT_ID']]
        df = df.drop(columns=cols_to_drop, errors='ignore')

        # Identify feature columns (exclude PATNO and EVENT_ID)
        feat_cols = [c for c in df.columns if c not in ['PATNO', 'EVENT_ID']]

        # Convert to numeric and fill NaN
        for col in feat_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # Scale features within each patient group
        scaler_seq = StandardScaler()
        for col in feat_cols:
            df[col] = df.groupby('PATNO')[col].transform(
                lambda x: scaler_seq.fit_transform(x.fillna(0).values.reshape(-1, 1)).flatten()
            )

        df[feat_cols] = df[feat_cols].fillna(0).astype('float32')

        return df, feat_cols

    def create_sequences(df, feat_cols, max_seq_len=10):
        """Create fixed-length sequences for each patient"""
        seqs, pat_list = [], []
        for pid, grp in df.groupby('PATNO'):
            data = grp.sort_values('EVENT_ID')[feat_cols].to_numpy(dtype=np.float32)
            t = torch.tensor(data)
            if t.size(0) < max_seq_len:
                # Pad with zeros at the end
                pad = torch.zeros(max_seq_len - t.size(0), t.size(1))
                t = torch.cat([t, pad], dim=0)
            else:
                # Truncate to max_seq_len
                t = t[:max_seq_len]
            seqs.append(t)
            pat_list.append(pid)
        return seqs, pat_list

    # Process all non-motor modalities
    dfs = [df_cog, df_psych, df_sleep, df_auto, df_sens]
    processed, feature_cols_list = [], []

    for i, df in enumerate(dfs):
        print(f"   Processing non-motor modality {i+1}/5...")
        df2, fcols = preprocess_keep_numeric(df, non_numeric_cols)
        processed.append(df2)
        feature_cols_list.append(fcols)
        print(f"     ✓ Features: {len(fcols)}")

    # Create sequences for each modality
    seqs_list, patno_lists = [], []
    for df, cols in zip(processed, feature_cols_list):
        seqs, pats = create_sequences(df, cols)
        seqs_list.append(seqs)
        patno_lists.append(pats)

    # Get union of all patients across non-motor modalities
    all_patnos_nm = sorted(set().union(*[set(pl) for pl in patno_lists]))

    # Align sequences to the union of all non-motor patients
    aligned_seqs = []
    for i, (mod_seqs, mod_patnos) in enumerate(zip(seqs_list, patno_lists)):
        pid_to_seq = dict(zip(mod_patnos, mod_seqs))

        # Get dimensions for padding
        if mod_seqs:
            feat_dim = mod_seqs[0].size(1)
            seq_len = mod_seqs[0].size(0)
        else:
            feat_dim = len(feature_cols_list[i])
            seq_len = 10  # Default max_seq_len

        aligned = []
        for pid in all_patnos_nm:
            if pid in pid_to_seq:
                aligned.append(pid_to_seq[pid])
            else:
                # Create a zero tensor for patients missing this modality
                aligned.append(torch.zeros(seq_len, feat_dim, dtype=torch.float32))

        aligned_seqs.append(aligned)

    nonmotor_patients = set(all_patnos_nm)
    print(f"   ✓ Non-motor: {len(nonmotor_patients)} patients")
    input_dims_nm = [len(cols) for cols in feature_cols_list]
    print(f"   ✓ Non-motor feature dimensions: {input_dims_nm}")

    # Load imaging data
    print("\n3. Loading imaging data...")
    df_dti = pd.read_parquet('dti.parquet', engine='pyarrow')
    df_mri = pd.read_parquet('mri.parquet', engine='pyarrow')

    # DTI Processing - Pivot from long to wide format
    # DTI has: PATNO, Measure (E1/E2/E3), Tissue (SN), ROI1-6, REF1-2
    print("   Processing DTI data...")
    value_cols = ['ROI1', 'ROI2', 'ROI3', 'ROI4', 'ROI5', 'ROI6', 'REF1', 'REF2']

    # Pivot: each Measure (E1, E2, E3) becomes separate columns for each ROI
    dti_pivot = df_dti.pivot_table(
        index='PATNO',
        columns='Measure',
        values=value_cols,
        aggfunc='mean'  # Average if multiple tissue types per patient
    )

    # Flatten column names: (ROI1, E1) -> ROI1_E1
    dti_pivot.columns = ['_'.join(map(str, col)) for col in dti_pivot.columns.values]
    dti_pivot = dti_pivot.reset_index()

    # Remove rows with too many missing values
    dti_cols = [col for col in dti_pivot.columns if col != 'PATNO']
    dti_pivot = dti_pivot.dropna(thresh=len(dti_cols) * 0.5)  # Keep rows with at least 50% non-null

    print(f"   ✓ DTI: {len(dti_pivot)} patients, {len(dti_cols)} features")

    # MRI Processing - Select anatomical features and latest visit
    print("   Processing MRI data...")
    anatomical_cols = [col for col in df_mri.columns if col.startswith((
        'lh_', 'rh_', 'Left_', 'Right_',
        'BrainSeg', 'Cortex', 'CC_', 'Cerebellum', 'Ventricle',
        'Thalamus', 'Caudate', 'Putamen', 'Pallidum',
        'Hippocampus', 'Amygdala', 'Accumbens'
    )) and col not in ['lhSurfaceHoles', 'rhSurfaceHoles']]

    # Select base columns and anatomical features
    mri_features = df_mri[['PATNO', 'EVENT_ID'] + anatomical_cols].copy()

    # Get latest visit per patient
    mri_features = mri_features.sort_values(['PATNO', 'EVENT_ID']).groupby('PATNO').tail(1).reset_index(drop=True)

    # Drop EVENT_ID after selection
    mri_features = mri_features.drop(columns=['EVENT_ID'])

    print(f"   ✓ MRI: {len(mri_features)} patients, {len(anatomical_cols)} features")

    # Merge DTI and MRI on PATNO
    df_imaging = dti_pivot.merge(mri_features, on='PATNO', how='inner')
    print(f"   ✓ Combined imaging: {len(df_imaging)} patients after merge")

    # Identify all imaging feature columns (exclude ID columns)
    id_cols = ['PATNO', 'EVENT_ID', 'VISCODE', 'EXAMDATE']
    imaging_cols = [col for col in df_imaging.columns if col not in id_cols]

    # Remove columns that are all NaN or have too many missing values
    missing_pct = df_imaging[imaging_cols].isnull().sum() / len(df_imaging)
    valid_cols = missing_pct[missing_pct < 0.8].index.tolist()  # Keep columns with <80% missing

    if len(valid_cols) < len(imaging_cols):
        print(f"   ⚠ Removed {len(imaging_cols) - len(valid_cols)} columns with >80% missing values")
        imaging_cols = valid_cols

    print(f"   ✓ Found {len(imaging_cols)} valid imaging features")

    # Convert to numeric and handle any remaining non-numeric values
    for col in imaging_cols:
        df_imaging[col] = pd.to_numeric(df_imaging[col], errors='coerce')

    # Impute and scale imaging features
    imputer_imaging = SimpleImputer(strategy='mean')
    imaging_imputed = imputer_imaging.fit_transform(df_imaging[imaging_cols])

    scaler_imaging = StandardScaler()
    imaging_scaled = scaler_imaging.fit_transform(imaging_imputed)

    # Update dataframe with scaled values
    df_imaging[imaging_cols] = imaging_scaled.astype(np.float32)

    imaging_patients = set(df_imaging['PATNO'].unique())
    print(f"   ✓ Imaging: {len(imaging_patients)} patients, {len(imaging_cols)} features")

    # Determine common patients and labels
    common_patients = motor_patients & nonmotor_patients & imaging_patients
    print(f"\n4. Common patients across all modalities: {len(common_patients)}")

    # Create labels
    print("\n5. Creating labels...")
    visit_phase_mapping = {
        'BL':'early', 'R17':'early', 'SC':'early', 'R16':'early', 'R15':'early',
        'R13':'early', 'LOG':'early', 'R08':'early', 'R10':'early', 'R12':'early',
        'R06':'early', 'R04':'early', 'R01':'early', 'PW':'early',
        'TRANS':'mid', 'V01':'mid', 'V02':'mid', 'V03':'mid', 'V04':'mid',
        'V05':'mid', 'V06':'mid', 'ST':'mid', 'RS1':'mid', 'R21':'mid',
        'U02':'mid', 'U01':'mid', 'R19':'mid', 'R20':'mid', 'R14':'mid',
        'V07':'late', 'V08':'late', 'V09':'late', 'V10':'late', 'V11':'late',
        'V12':'late', 'V13':'late', 'V14':'late', 'V15':'late', 'V16':'late',
        'V17':'late', 'V18':'late', 'V19':'late', 'V20':'late', 'V21':'late',
        'V22':'late', 'R18':'late'
    }
    stage_mapping = {'early': 0, 'mid': 1, 'late': 2}

    # Use all unique events across non-motor data
    all_events = pd.concat([df[['PATNO','EVENT_ID']] for df in processed]).drop_duplicates()
    all_events['stage_str'] = all_events['EVENT_ID'].map(visit_phase_mapping)
    latest = all_events.sort_values('EVENT_ID').groupby('PATNO').tail(1).reset_index(drop=True)
    labels_df = latest[['PATNO','stage_str']].copy()
    labels_df['label'] = labels_df['stage_str'].map(stage_mapping)
    labels_df = labels_df[labels_df['PATNO'].isin(common_patients)]

    common_patients = sorted(list(common_patients))

    # Filter and align data for final common patients
    df_motor_filtered = df_motor[df_motor['PATNO'].isin(common_patients)].sort_values('PATNO')
    df_imaging_filtered = df_imaging[df_imaging['PATNO'].isin(common_patients)].sort_values('PATNO')

    # Re-align nonmotor sequences
    nm_patient_to_idx = {pid: i for i, pid in enumerate(all_patnos_nm)}
    aligned_nm_seqs_filtered = []

    for pid in common_patients:
        idx = nm_patient_to_idx[pid]
        patient_tuple = tuple(modality_list[idx] for modality_list in aligned_seqs)
        aligned_nm_seqs_filtered.append(patient_tuple)

    print(f"   ✓ Labels created for {len(labels_df)} common patients")
    print(f"   ✓ Label distribution: {dict(labels_df['label'].value_counts())}")

    return {
        'motor_df': df_motor_filtered,
        'motor_cols': motor_cols,
        'nonmotor_seqs': aligned_nm_seqs_filtered,
        'imaging_df': df_imaging_filtered,
        'imaging_cols': imaging_cols,
        'patient_ids': common_patients,
        'labels_df': labels_df,
        'input_dims_nm': input_dims_nm,
        'embed_dims_nm': [16, 16, 12, 12, 8]
    }
# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================

def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss, correct, total = 0, 0, 0

    for motor_x, nonmotor_x, imaging_x, patient_ids in tqdm(dataloader, desc="Training", leave=False):
        motor_x = motor_x.to(device)
        nonmotor_x = tuple(x.to(device) for x in nonmotor_x)
        imaging_x = imaging_x.to(device)

        # Get labels using the patient IDs and the dataset's label map
        labels = torch.tensor([dataloader.dataset.dataset.label_map[pid] for pid in patient_ids], dtype=torch.long).to(device)

        optimizer.zero_grad()
        logits, _, _ = model(motor_x, nonmotor_x, imaging_x)
        loss = criterion(logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return total_loss / len(dataloader), correct / total


def validate_epoch(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds, all_labels, all_probs = [], [], []

    with torch.no_grad():
        for motor_x, nonmotor_x, imaging_x, patient_ids in tqdm(dataloader, desc="Validating", leave=False):
            motor_x = motor_x.to(device)
            nonmotor_x = tuple(x.to(device) for x in nonmotor_x)
            imaging_x = imaging_x.to(device)

            labels = torch.tensor([dataloader.dataset.dataset.label_map[pid] for pid in patient_ids], dtype=torch.long).to(device)

            logits, _, _ = model(motor_x, nonmotor_x, imaging_x)
            loss = criterion(logits, labels)
            total_loss += loss.item()

            probs = F.softmax(logits, dim=1)
            preds = torch.argmax(logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)

    return avg_loss, accuracy, all_preds, all_labels, np.array(all_probs)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    print("\n" + "="*70)
    print("PARKINSON'S DISEASE COMPLETE TRAINING PIPELINE")
    print("="*70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nDevice: {device}")

    # Step 1: Load and preprocess data
    data_dict = load_and_preprocess_all_data()

    # Step 2: Create dataset
    print("\n" + "="*70)
    print("CREATING DATASET")
    print("="*70)

    dataset = MultimodalParkinsonsDataset(
        motor_df=data_dict['motor_df'],
        nonmotor_seqs=data_dict['nonmotor_seqs'],
        imaging_df=data_dict['imaging_df'],
        patient_ids=data_dict['patient_ids'],
        labels_df=data_dict['labels_df']
    )
    dataset.motor_cols = data_dict['motor_cols']
    dataset.imaging_cols = data_dict['imaging_cols']

    # Step 3: Load master split
    print("\n" + "="*70)
    print("LOADING MASTER PATIENT SPLIT")
    print("="*70)

    try:
        with open('master_patient_split.pkl', 'rb') as f:
            split_data = pickle.load(f)
    except FileNotFoundError:
        print("Error: 'master_patient_split.pkl' not found. Please run the prerequisite script.")
        return

    train_patients = set(split_data['train_patients'])
    val_patients = set(split_data['val_patients'])
    test_patients = set(split_data['test_patients'])

    # Filter indices based on patients present in the final, common dataset
    train_indices = [i for i, pid in enumerate(dataset.patient_ids) if pid in train_patients]
    val_indices = [i for i, pid in enumerate(dataset.patient_ids) if pid in val_patients]
    test_indices = [i for i, pid in enumerate(dataset.patient_ids) if pid in test_patients]

    print(f"Train: {len(train_indices)} | Val: {len(val_indices)} | Test: {len(test_indices)}")

    # Step 4: Create data loaders
    BATCH_SIZE = 8

    train_subset = torch.utils.data.Subset(dataset, train_indices)
    val_subset = torch.utils.data.Subset(dataset, val_indices)
    test_subset = torch.utils.data.Subset(dataset, test_indices)

    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)
    test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

    # Step 5: Load encoders with dimension checking
    print("\n" + "="*70)
    print("LOADING PRE-TRAINED ENCODERS")
    print("="*70)

    # Motor encoder (input_dim=14, latent_dim=16)
    motor_encoder = TransformerMotorEncoder(input_dim=len(data_dict['motor_cols']), latent_dim=16).to(device)
    motor_loaded = load_encoder_with_check(motor_encoder, 'motor_encoder_weights.pth', device)

    # Non-motor encoder with current data dimensions
    nonmotor_encoder = NonMotorEncoder(data_dict['input_dims_nm'], data_dict['embed_dims_nm'], common_dim=16).to(device)
    nonmotor_loaded = load_encoder_with_check(nonmotor_encoder, 'nonmotor_encoder_weights.pth', device)

    # Imaging encoder (input_dim=len(imaging_cols), latent_dim=16)
    imaging_encoder = ImagingEncoder(len(data_dict['imaging_cols']), latent_dim=16).to(device)
    imaging_loaded = load_encoder_with_check(imaging_encoder, 'imaging_encoder_weights.pth', device)

    if not (motor_loaded and nonmotor_loaded and imaging_loaded):
        print("\n⚠ Warning: Some encoders using random initialization due to dimension mismatch or missing files.")
        print("  Training will proceed with uninitialized or partially initialized weights.")

    # Step 6: Create fused encoder
    print("\n" + "="*70)
    print("CREATING FUSED ENCODER")
    print("="*70)

    fused_encoder = UnifiedFusedEncoder(motor_encoder, nonmotor_encoder, imaging_encoder, latent_dim=16, freeze_encoders=True).to(device)
    print(f"✓ Fused encoder created (output dim: {fused_encoder.get_embedding_dim()})")

    # Step 7: Create classifier
    classifier = PDStageClassifier(input_dim=48, hidden_dim=128, num_classes=3, dropout=0.4).to(device)
    model = CompletePDModel(fused_encoder, classifier).to(device)

    # Step 8: Setup training
    print("\n" + "="*70)
    print("TRAINING SETUP")
    print("="*70)

    train_labels = [dataset.label_map[dataset.patient_ids[i]] for i in train_indices]
    class_counts = np.bincount(train_labels, minlength=3)
    # Calculate inverse class frequency weights
    class_weights = torch.FloatTensor(len(train_labels) / (3 * class_counts)).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)

    # Only optimize the classifier parameters and fusion parameters if encoders are frozen
    optimizer_params = [
        {'params': model.classifier.parameters()},
        {'params': model.fused_encoder.fusion.parameters()}
    ]
    if not fused_encoder.freeze_encoders:
        optimizer_params.append({'params': fused_encoder.motor_encoder.parameters()})
        optimizer_params.append({'params': fused_encoder.nonmotor_encoder.parameters()})
        optimizer_params.append({'params': fused_encoder.imaging_encoder.parameters()})

    optimizer = torch.optim.Adam(optimizer_params, lr=1e-3)

    print(f"Class counts: {class_counts}")
    print(f"Class weights: {class_weights.cpu().numpy()}")

    # Step 9: Training loop
    print("\n" + "="*70)
    print("TRAINING")
    print("="*70)

    best_val_acc = 0
    patience_counter = 0
    patience = 15
    num_epochs = 100

    train_losses, val_losses = [], []
    train_accs, val_accs = [], []

    for epoch in range(num_epochs):
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_acc, _, _, _ = validate_epoch(model, val_loader, criterion, device)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        print(f"Epoch {epoch+1:3d}/{num_epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc*100:5.1f}% | Val Loss: {val_loss:.4f} Acc: {val_acc*100:5.1f}%")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss
            }, 'best_complete_model.pth')
            print(f"  ✓ Best model saved (Val Acc: {val_acc*100:.1f}%)")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Step 10: Load best model and evaluate

    print("\n" + "="*70)
    print("FINAL EVALUATION ON TEST SET")
    print("="*70)

    try:
        checkpoint = torch.load('best_complete_model.pth', map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
    except FileNotFoundError:
        print("Warning: Best model checkpoint not found. Evaluating with last epoch weights.")

    test_loss, test_acc, test_preds, test_labels, test_probs = validate_epoch(model, test_loader, criterion, device)

    # Filter metrics to only include classes present in the test set
    stage_names = ['Early', 'Mid', 'Late']
    unique_labels = np.unique(test_labels)

    precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_preds,
                                                                   labels=[0, 1, 2], average=None, zero_division=0)
    macro_f1 = f1_score(test_labels, test_preds, average='macro', zero_division=0)

    # AUC calculation
    try:
        if test_probs.shape[1] == 3:
            auc = roc_auc_score(test_labels, test_probs, multi_class='ovr', average='macro', labels=[0, 1, 2])
        else:
            auc = 0.0
            print("Warning: AUC calculation skipped due to missing class probabilities")
    except Exception as e:
        auc = 0.0
        print(f"Warning: AUC calculation failed: {e}")

    cm = confusion_matrix(test_labels, test_preds, labels=[0, 1, 2])


    print(f"\nTest Results:")
    print(f"  Accuracy: {test_acc*100:.1f}%")
    print(f"  Macro F1: {macro_f1*100:.1f}%")
    print(f"  AUC: {auc:.3f}")

    print(f"\nPer-Class Metrics:")
    for i, stage in enumerate(stage_names):
        print(f"  {stage}:")
        print(f"    Precision: {precision[i]*100:.1f}%")
        print(f"    Recall: {recall[i]*100:.1f}%")
        print(f"    F1-Score: {f1[i]*100:.1f}%")
        print(f"    Support: {support[i]}")

    print(f"\nConfusion Matrix:")
    print(cm)

    # Step 11: Visualizations (Completed Code)
    print("\n" + "="*70)
    print("GENERATING VISUALIZATIONS")
    print("="*70)

    # Confusion matrix
    cm_pct = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8) * 100
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Blues',
                xticklabels=stage_names, yticklabels=stage_names,
                cbar_kws={'label': 'Percentage (%)'})
    plt.title('Test Set Confusion Matrix (%)', fontsize=14, fontweight='bold')
    plt.ylabel('True Stage', fontsize=12)
    plt.xlabel('Predicted Stage', fontsize=12)
    plt.tight_layout()
    plt.savefig('confusion_matrix_test.png', dpi=300, bbox_inches='tight')
    print("✓ Saved: confusion_matrix_test.png")
    plt.close()

    # Training curves
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    epochs = range(1, len(train_losses) + 1)

    # Loss Plot
    axes[0].plot(epochs, train_losses, label='Train', linewidth=2)
    axes[0].plot(epochs, val_losses, label='Validation', linewidth=2, linestyle='--')
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training vs Validation Loss', fontsize=14, fontweight='bold')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3)

    # Accuracy Plot (Completes the truncated code from the prompt)
    axes[1].plot(epochs, train_accs, label='Train', linewidth=2)
    axes[1].plot(epochs, val_accs, label='Validation', linewidth=2, linestyle='--')
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy', fontsize=12)
    axes[1].set_title('Training vs Validation Accuracy', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=10)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')
    print("✓ Saved: training_curves.png")
    plt.close(fig)

    # Step 12: SHAP Analysis for Feature Importance
    if SHAP_AVAILABLE:
        print("\n" + "="*70)
        print("SHAP FEATURE IMPORTANCE ANALYSIS (Latent Space)")
        print("="*70)

        try:
            # 1. Collect all fused embeddings from the test set
            model.eval()
            test_fused_embs = []
            for motor_x, nonmotor_x, imaging_x, _ in test_loader:
                motor_x = motor_x.to(device)
                nonmotor_x = tuple(x.to(device) for x in nonmotor_x)
                imaging_x = imaging_x.to(device)
                _, fused_emb, _ = model.fused_encoder(motor_x, nonmotor_x, imaging_x)
                test_fused_embs.append(fused_emb.cpu().numpy())

            test_fused_embs = np.concatenate(test_fused_embs, axis=0)

            # 2. Define latent feature names for interpretability
            feature_names = [f"Motor_Emb_{i+1}" for i in range(16)] + \
                            [f"NM_Emb_{i+1}" for i in range(16)] + \
                            [f"Img_Emb_{i+1}" for i in range(16)]

            # 3. Create a callable prediction function for SHAP (Classifier only)
            def classifier_predict(X):
                # X is numpy array of shape (N, 48)
                X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
                with torch.no_grad():
                    logits = model.classifier(X_tensor)
                    return logits.cpu().numpy()

            # 4. Use KernelExplainer (suitable for complex, non-linear models)
            # Use a sample of the test set for background for faster computation
            background_data = shap.maskers.Independent(test_fused_embs, max_samples=100)
            explainer = shap.KernelExplainer(classifier_predict, background_data)

            # 5. Calculate SHAP values for the entire test set
            # This can be slow; consider using a subset of the test_fused_embs if needed
            print("  Calculating SHAP values (This may take some time)...")
            shap_values_list = explainer.shap_values(test_fused_embs)

            # 6. Generate Summary Plot
            # shap_values_list is a list of arrays (one for each class: 0, 1, 2)
            shap.summary_plot(shap_values_list, test_fused_embs, feature_names=feature_names,
                              class_names=stage_names, title="SHAP Summary Plot (Fused Latent Features)",
                              show=False)
            plt.savefig('shap_summary_all_classes.png', dpi=300, bbox_inches='tight')
            print("✓ Saved: shap_summary_all_classes.png (for latent features)")
            plt.close()

        except Exception as e:
            print(f"⚠ SHAP analysis failed: {e}")
            print("  Ensure all dependencies are correctly installed and the model is not too large for SHAP.")

    print("\n" + "="*70)
    print("PIPELINE COMPLETE")
    print("="*70)

    # ============================================================================
    # REAL INFERENCE ON SAMPLE TEST PATIENTS
    # ============================================================================

    print("\n" + "="*70)
    print("REAL INFERENCE ON SAMPLE TEST PATIENTS")
    print("="*70)

    # Select a few sample patients from the test set for demonstration
    # Try to pick one from each stage if available
    test_patient_ids = dataset.patient_ids
    test_labels = [dataset.label_map[pid] for pid in test_patient_ids]

    # Find indices for each stage
    early_indices = [i for i, lbl in enumerate(test_labels) if lbl == 0][:1]  # Take first early
    mid_indices = [i for i, lbl in enumerate(test_labels) if lbl == 1][:1]    # Take first mid
    late_indices = [i for i, lbl in enumerate(test_labels) if lbl == 2][:1]   # Take first late

    sample_indices = early_indices + mid_indices + late_indices
    sample_patients = [test_patient_ids[i] for i in sample_indices]

    stage_names = ['Early Stage', 'Mid Stage', 'Late Stage']

    model.eval()
    with torch.no_grad():
        for idx, (pat_idx, stage_name) in enumerate(zip(sample_indices, stage_names)):
            if pat_idx >= len(test_subset):
                continue  # Skip if index out of range

            # Get patient data
            motor_x, nonmotor_x, imaging_x, patno = test_subset[pat_idx]
            motor_x = motor_x.unsqueeze(0).to(device)  # Add batch dim
            nonmotor_x = tuple(x.unsqueeze(0).to(device) for x in nonmotor_x)
            imaging_x = imaging_x.unsqueeze(0).to(device)

            # Run model
            logits, fused_emb, individual_embs = model(motor_x, nonmotor_x, imaging_x)
            probs = F.softmax(logits, dim=1).cpu().numpy()[0]
            pred_stage = np.argmax(probs)
            confidence = probs[pred_stage] * 100

            # Attention weights
            attn_weights = individual_embs['attention_weights'].cpu().numpy()[0]  # Shape can vary: (num_heads, seq_len, seq_len), (seq_len, seq_len), or (seq_len,)
            # Average to get modality importance weights
            if attn_weights.ndim == 3:
                modality_weights = attn_weights.mean(axis=0).mean(axis=1)  # Average over heads and target positions
            elif attn_weights.ndim == 2:
                modality_weights = attn_weights.mean(axis=1)  # Average over target positions
            else:  # ndim == 1
                modality_weights = attn_weights  # Already the weights
            modality_weights = modality_weights / modality_weights.sum()  # Normalize to sum to 1

            print(f"\n{'='*80}")
            print(f"INFERENCE REPORT #{idx+1}: {stage_name.upper()}")
            print(f"{'='*80}")

            print(f"PATIENT ID: {patno}")
            print(f"VISIT: Test Set Sample")
            print(f"Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}")

            print(f"\nMODALITY FEATURES EXTRACTED:")
            print(f"[1] MOTOR FEATURES ({len(data_dict['motor_cols'])} dimensions): Latest visit data")
            print(f"[2] NON-MOTOR FEATURES: Sequence data across 5 domains")
            print(f"[3] IMAGING FEATURES ({len(data_dict['imaging_cols'])} dimensions): MRI/DTI data")

            print(f"\nENCODER OUTPUTS (Latent Representations):")
            motor_emb = individual_embs['motor'].cpu().numpy()[0][:5]  # First 5 dims
            nm_emb = individual_embs['nonmotor'].cpu().numpy()[0][:5]
            img_emb = individual_embs['imaging'].cpu().numpy()[0][:5]
            print(f"  Motor Encoder:     [16-dim] → [{', '.join(f'{x:.2f}' for x in motor_emb)}, ...]")
            print(f"  Non-Motor Encoder: [16-dim] → [{', '.join(f'{x:.2f}' for x in nm_emb)}, ...]")
            print(f"  Imaging Encoder:   [16-dim] → [{', '.join(f'{x:.2f}' for x in img_emb)}, ...]")
            print(f"  Fused Embedding:   [48-dim] → Ready for classification")

            # --- New Pretty Inference Output ---
            pred_label = stage_names[pred_stage].upper()
            diagnosis = f"{pred_label} PARKINSON'S DISEASE"
            modality_names = ["MOTOR", "NON-MOTOR", "IMAGING"]
            modality_weights_list = list(zip(modality_names, modality_weights))
            key_features = [
                "Motor symptom score (UPDRS-II)",
                "Cognitive / Sleep domain status",
                "MRI/DTI signal abnormalities"
            ]

            print_inference_report(
                patno=patno,
                visit="Test Visit",
                diagnosis=diagnosis,
                confidence=confidence,
                modality_weights=modality_weights_list,
                key_features=key_features
            )


    print(f"\n{'='*80}")
    print("REAL INFERENCE COMPLETE")
    print(f"{'='*80}")

if __name__ == '__main__':
    # Execute the main function when the script is run
    main()